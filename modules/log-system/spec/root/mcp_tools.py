#!/usr/bin/env python3
"""
í†µí•© ë¡œê·¸ ìˆ˜ì§‘ê¸° - MCP (Model Context Protocol) ì—°ë™ ë„êµ¬ (ì™„ì„±ë³¸)
LLMì´ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•  ìˆ˜ ìˆëŠ” ë„êµ¬ë“¤
"""

import json
import time
import re
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union
from collections import defaultdict, Counter
from dataclasses import dataclass
import sqlite3
import os

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    import sys
    sys.path.append('../../python')
    from storage import LogStorage, create_storage
    from main import Config, load_config_with_overrides
except ImportError:
    print("ë¡œì»¬ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.")


@dataclass
class LogAnalysis:
    """ë¡œê·¸ ë¶„ì„ ê²°ê³¼"""
    total_logs: int
    timerange: str
    error_rate: float
    top_errors: List[Dict]
    performance_issues: List[Dict]
    trends: Dict[str, Any]
    recommendations: List[str]


class MCPLogAnalyzer:
    """MCPìš© ë¡œê·¸ ë¶„ì„ê¸°"""
    
    def __init__(self, config_path: str = None):
        self.config = load_config_with_overrides(config_path)
        self.storage = create_storage(
            db_path=self.config.storage.db_path,
            max_size_mb=self.config.storage.max_size_mb,
            max_days=self.config.storage.max_days
        )
    
    def get_recent_errors(self, minutes: int = 30) -> List[Dict]:
        """ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ ì¡°íšŒ"""
        try:
            # ì‹œê°„ ê³„ì‚°
            since_time = datetime.now() - timedelta(minutes=minutes)
            
            # ì—ëŸ¬ ë¡œê·¸ ì¡°íšŒ
            return self.storage.query_logs(
                levels=['ERROR', 'FATAL'],
                since=since_time.isoformat(),
                limit=100
            )
        except Exception as e:
            return [{'error': str(e)}]
    
    def find_slow_queries(self, threshold_ms: int = 1000) -> List[Dict]:
        """ìŠ¬ë¡œìš° ì¿¼ë¦¬ ì¡°íšŒ"""
        try:
            # ìµœê·¼ 1ì‹œê°„ ë™ì•ˆì˜ ë¡œê·¸ ì¡°íšŒ
            since_time = datetime.now() - timedelta(hours=1)
            
            all_logs = self.storage.query_logs(
                since=since_time.isoformat(),
                limit=1000
            )
            
            # ìŠ¬ë¡œìš° ì¿¼ë¦¬ í•„í„°ë§
            slow_queries = []
            for log in all_logs:
                duration = log.get('metadata', {}).get('duration_ms', 0)
                if duration >= threshold_ms:
                    slow_queries.append({
                        'timestamp': log['timestamp'],
                        'source': log['source'],
                        'message': log['message'],
                        'duration_ms': duration,
                        'query': log.get('metadata', {}).get('query', log['message'])
                    })
            
            return slow_queries
        except Exception as e:
            return [{'error': str(e)}]
    
    def trace_request(self, trace_id: str) -> Dict:
        """íŠ¸ë ˆì´ìŠ¤ IDë¡œ ìš”ì²­ ì¶”ì """
        try:
            # íŠ¸ë ˆì´ìŠ¤ IDë¡œ ë¡œê·¸ ì¡°íšŒ
            logs = self.storage.query_logs(
                trace_id=trace_id,
                limit=100
            )
            
            if not logs:
                return {'error': f'íŠ¸ë ˆì´ìŠ¤ ID {trace_id}ì— ëŒ€í•œ ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}
            
            # íƒ€ì„ë¼ì¸ ì •ë ¬
            sorted_logs = sorted(logs, key=lambda x: x['timestamp'])
            
            return {
                'trace_id': trace_id,
                'log_count': len(logs),
                'start_time': sorted_logs[0]['timestamp'],
                'end_time': sorted_logs[-1]['timestamp'],
                'logs': sorted_logs
            }
        except Exception as e:
            return {'error': str(e)}
    
    def analyze_error_pattern(self, error_message: str, hours: int = 24) -> Dict:
        """ì—ëŸ¬ íŒ¨í„´ ë¶„ì„"""
        try:
            since_time = datetime.now() - timedelta(hours=hours)
            
            # ìœ ì‚¬í•œ ì—ëŸ¬ ë©”ì‹œì§€ ê²€ìƒ‰
            error_logs = self.storage.query_logs(
                levels=['ERROR', 'FATAL'],
                since=since_time.isoformat(),
                limit=500
            )
            
            # íŒ¨í„´ ë§¤ì¹­
            similar_errors = []
            for log in error_logs:
                if error_message.lower() in log['message'].lower():
                    similar_errors.append(log)
            
            return {
                'pattern': error_message,
                'timerange_hours': hours,
                'similar_errors_count': len(similar_errors),
                'similar_errors': similar_errors[:20]  # ìµœëŒ€ 20ê°œ
            }
        except Exception as e:
            return {'error': str(e)}
    
    def debug_session(self, since: str = "5m") -> Dict:
        """ë””ë²„ê¹… ì„¸ì…˜ ì •ë³´ ìˆ˜ì§‘"""
        try:
            # ì‹œê°„ íŒŒì‹±
            if since.endswith('m'):
                minutes = int(since[:-1])
                since_time = datetime.now() - timedelta(minutes=minutes)
            elif since.endswith('h'):
                hours = int(since[:-1])
                since_time = datetime.now() - timedelta(hours=hours)
            else:
                since_time = datetime.now() - timedelta(minutes=5)
            
            # ëª¨ë“  ë¡œê·¸ ì¡°íšŒ
            all_logs = self.storage.query_logs(
                since=since_time.isoformat(),
                limit=1000
            )
            
            # ë¶„ë¥˜
            errors = [log for log in all_logs if log['level'] in ['ERROR', 'FATAL']]
            warnings = [log for log in all_logs if log['level'] == 'WARN']
            
            return {
                'timerange': since,
                'total_logs': len(all_logs),
                'errors': len(errors),
                'warnings': len(warnings),
                'recent_errors': errors[:10],
                'recent_warnings': warnings[:10]
            }
        except Exception as e:
            return {'error': str(e)}
        
    def _generate_alerts(self, stats: Dict) -> List[Dict]:
        """í†µê³„ ê¸°ë°˜ ì•Œë¦¼ ìƒì„±"""
        alerts = []
        
        basic = stats.get('basic', {})
        by_level = stats.get('by_level', {})
        trends = self._analyze_trends(stats)
        
        # ì—ëŸ¬ìœ¨ ì•Œë¦¼
        total_logs = basic.get('total_logs', 0)
        if total_logs > 0:
            error_count = by_level.get('ERROR', 0) + by_level.get('FATAL', 0)
            error_rate = error_count / total_logs
            
            if error_rate > 0.1:
                alerts.append({
                    'type': 'high_error_rate',
                    'severity': 'critical',
                    'message': f'ì—ëŸ¬ìœ¨ì´ {error_rate*100:.1f}%ë¡œ ë†’ìŠµë‹ˆë‹¤',
                    'value': error_rate,
                    'threshold': 0.1
                })
            elif error_rate > 0.05:
                alerts.append({
                    'type': 'elevated_error_rate',
                    'severity': 'warning',
                    'message': f'ì—ëŸ¬ìœ¨ì´ {error_rate*100:.1f}%ë¡œ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤',
                    'value': error_rate,
                    'threshold': 0.05
                })
                
        # ì—ëŸ¬ ì¦ê°€ íŠ¸ë Œë“œ ì•Œë¦¼
        if trends.get('error_trend') == 'increasing':
            alerts.append({
                'type': 'increasing_errors',
                'severity': 'warning',
                'message': 'ì—ëŸ¬ ë°œìƒì´ ì¦ê°€ ì¶”ì„¸ì…ë‹ˆë‹¤',
                'trend': 'increasing'
            })
            
        return alerts
        
    def _analyze_search_patterns(self, logs: List[Dict]) -> Dict:
        """ê²€ìƒ‰ ê²°ê³¼ íŒ¨í„´ ë¶„ì„"""
        patterns = {
            'sources': Counter([log['source'] for log in logs]),
            'levels': Counter([log['level'] for log in logs]),
            'time_distribution': defaultdict(int),
            'common_keywords': Counter()
        }
        
        # ì‹œê°„ëŒ€ë³„ ë¶„í¬
        for log in logs:
            try:
                timestamp = datetime.fromisoformat(log['timestamp'].replace('Z', '+00:00'))
                hour = timestamp.strftime('%H:00')
                patterns['time_distribution'][hour] += 1
            except:
                pass
                
        # ê³µí†µ í‚¤ì›Œë“œ ì¶”ì¶œ
        all_messages = ' '.join([log['message'] for log in logs])
        words = re.findall(r'\w+', all_messages.lower())
        patterns['common_keywords'] = Counter(words).most_common(10)
        
        return patterns
        
    def _create_timeline(self, logs: List[Dict]) -> List[Dict]:
        """ë¡œê·¸ íƒ€ì„ë¼ì¸ ìƒì„±"""
        sorted_logs = sorted(logs, key=lambda x: x['timestamp'])
        
        timeline = []
        for i, log in enumerate(sorted_logs):
            timeline_entry = {
                'sequence': i + 1,
                'timestamp': log['timestamp'],
                'source': log['source'],
                'level': log['level'],
                'message': log['message']
            }
            
            # ì´ì „ ë¡œê·¸ì™€ì˜ ì‹œê°„ ê°„ê²©
            if i > 0:
                try:
                    current_time = datetime.fromisoformat(log['timestamp'].replace('Z', '+00:00'))
                    prev_time = datetime.fromisoformat(sorted_logs[i-1]['timestamp'].replace('Z', '+00:00'))
                    gap_ms = int((current_time - prev_time).total_seconds() * 1000)
                    timeline_entry['gap_from_previous_ms'] = gap_ms
                except:
                    pass
                    
            timeline.append(timeline_entry)
            
        return timeline


# MCP ë„êµ¬ í•¨ìˆ˜ë“¤ - LLMì´ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë“¤
def get_recent_logs(minutes: int = 30, limit: int = 50, levels: List[str] = None, sources: List[str] = None) -> Dict:
    """
    ìµœê·¼ ë¡œê·¸ ì¡°íšŒ (ëª¨ë“  ë ˆë²¨)
    
    Args:
        minutes: ì¡°íšŒí•  ì‹œê°„ ë²”ìœ„ (ë¶„)
        limit: ìµœëŒ€ ì¡°íšŒí•  ë¡œê·¸ ìˆ˜
        levels: í•„í„°ë§í•  ë¡œê·¸ ë ˆë²¨ (ì˜ˆ: ['INFO', 'WARN', 'ERROR'])
        sources: í•„í„°ë§í•  ì†ŒìŠ¤ (ì˜ˆ: ['mcp_calls', 'http_traffic'])
        
    Returns:
        ìµœê·¼ ë¡œê·¸ ëª©ë¡ê³¼ ë¶„ì„ ì •ë³´
    """
    try:
        analyzer = MCPLogAnalyzer()
        
        # ì‹œê°„ ê³„ì‚°
        since_time = datetime.now() - timedelta(minutes=minutes)
        
        # ë¡œê·¸ ì¡°íšŒ
        logs = analyzer.storage.query_logs(
            levels=levels,
            sources=sources,
            since=since_time.isoformat(),
            limit=limit
        )
        
        if not logs:
            return {
                'success': True,
                'logs': [],
                'count': 0,
                'analysis': {
                    'message': f'ìµœê·¼ {minutes}ë¶„ê°„ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.',
                    'timerange_minutes': minutes,
                    'filters_applied': {
                        'levels': levels,
                        'sources': sources,
                        'limit': limit
                    }
                }
            }
            
        # ë¡œê·¸ ë¶„ì„
        log_levels = Counter([log.get('level', 'UNKNOWN') for log in logs])
        log_sources = Counter([log.get('source', 'unknown') for log in logs])
        
        analysis = {
            'total_logs': len(logs),
            'timerange_minutes': minutes,
            'log_by_level': dict(log_levels.most_common()),
            'log_by_source': dict(log_sources.most_common(5)),
            'most_recent': logs[0] if logs else None,
            'activity_summary': {
                'errors': log_levels.get('ERROR', 0) + log_levels.get('FATAL', 0),
                'warnings': log_levels.get('WARN', 0),
                'info_messages': log_levels.get('INFO', 0),
                'debug_messages': log_levels.get('DEBUG', 0),
                'error_rate': (log_levels.get('ERROR', 0) + log_levels.get('FATAL', 0)) / len(logs) if logs else 0
            },
            'filters_applied': {
                'levels': levels,
                'sources': sources,
                'limit': limit
            }
        }
        
        return {
            'success': True,
            'logs': logs,
            'count': len(logs),
            'analysis': analysis
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'count': 0,
            'logs': []
        }


def get_recent_errors(minutes: int = 30) -> Dict:
    """
    ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ ì¡°íšŒ
    
    Args:
        minutes: ì¡°íšŒí•  ì‹œê°„ ë²”ìœ„ (ë¶„)
        
    Returns:
        ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ ëª©ë¡ê³¼ ë¶„ì„ ì •ë³´
    """
    try:
        analyzer = MCPLogAnalyzer()
        errors = analyzer.get_recent_errors(minutes)
        
        if not errors or (len(errors) == 1 and 'error' in errors[0]):
            return {
                'success': False,
                'errors': errors,
                'count': 0,
                'analysis': {'message': 'ì—ëŸ¬ ë¡œê·¸ê°€ ì—†ê±°ë‚˜ ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'}
            }
            
        # ì—ëŸ¬ ë¶„ì„
        error_sources = Counter([e.get('source', 'unknown') for e in errors])
        error_types = Counter([e.get('error_type', 'unknown') for e in errors if e.get('error_type')])
        
        analysis = {
            'total_errors': len(errors),
            'timerange_minutes': minutes,
            'error_by_source': dict(error_sources.most_common(5)),
            'error_by_type': dict(error_types.most_common(5)),
            'most_recent': errors[0] if errors else None,
            'recommendations': []
        }
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        if len(errors) > 10:
            analysis['recommendations'].append(f"{minutes}ë¶„ê°„ {len(errors)}ê°œ ì—ëŸ¬ ë°œìƒ. ê¸´ê¸‰ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        most_common_source = error_sources.most_common(1)
        if most_common_source and most_common_source[0][1] > len(errors) * 0.5:
            analysis['recommendations'].append(f"'{most_common_source[0][0]}' ì†ŒìŠ¤ì—ì„œ ì—ëŸ¬ê°€ ì§‘ì¤‘ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
            
        return {
            'success': True,
            'errors': errors,
            'count': len(errors),
            'analysis': analysis
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'count': 0
        }


def find_slow_queries(threshold_ms: int = 1000) -> Dict:
    """
    ìŠ¬ë¡œìš° ì¿¼ë¦¬ ì°¾ê¸°
    
    Args:
        threshold_ms: ëŠë¦° ì¿¼ë¦¬ ê¸°ì¤€ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
        
    Returns:
        ìŠ¬ë¡œìš° ì¿¼ë¦¬ ëª©ë¡ê³¼ ë¶„ì„
    """
    try:
        analyzer = MCPLogAnalyzer()
        slow_queries = analyzer.find_slow_queries(threshold_ms)
        
        if not slow_queries or (len(slow_queries) == 1 and 'error' in slow_queries[0]):
            return {
                'success': False,
                'slow_queries': slow_queries,
                'count': 0,
                'analysis': {'message': 'ìŠ¬ë¡œìš° ì¿¼ë¦¬ê°€ ì—†ê±°ë‚˜ ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'}
            }
            
        # ë¶„ì„ ì •ë³´
        durations = [q['duration_ms'] for q in slow_queries]
        analysis = {
            'total_slow_queries': len(slow_queries),
            'threshold_ms': threshold_ms,
            'slowest_query_ms': max(durations) if durations else 0,
            'average_duration_ms': statistics.mean(durations) if durations else 0,
            'median_duration_ms': statistics.median(durations) if durations else 0,
            'recommendations': []
        }
        
        # ê¶Œì¥ì‚¬í•­
        if analysis['slowest_query_ms'] > 5000:
            analysis['recommendations'].append(f"ê°€ì¥ ëŠë¦° ì¿¼ë¦¬ê°€ {analysis['slowest_query_ms']}msì…ë‹ˆë‹¤. ì¦‰ì‹œ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
        if len(slow_queries) > 10:
            analysis['recommendations'].append(f"{len(slow_queries)}ê°œì˜ ìŠ¬ë¡œìš° ì¿¼ë¦¬ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì „ë°˜ì ì¸ DB ì„±ëŠ¥ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
        return {
            'success': True,
            'slow_queries': slow_queries,
            'count': len(slow_queries),
            'analysis': analysis
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'count': 0
        }


def trace_request(trace_id: str) -> Dict:
    """
    íŠ¹ì • íŠ¸ë ˆì´ìŠ¤ IDì˜ ìš”ì²­ ì¶”ì 
    
    Args:
        trace_id: ì¶”ì í•  íŠ¸ë ˆì´ìŠ¤ ID
        
    Returns:
        íŠ¸ë ˆì´ìŠ¤ ì •ë³´ì™€ ë¶„ì„
    """
    try:
        analyzer = MCPLogAnalyzer()
        trace_info = analyzer.trace_request(trace_id)
        
        if 'error' in trace_info:
            return {
                'success': False,
                'trace_id': trace_id,
                'error': trace_info['error']
            }
            
        return {
            'success': True,
            'trace_info': trace_info
        }
        
    except Exception as e:
        return {
            'success': False,
            'trace_id': trace_id,
            'error': str(e)
        }


def analyze_error_pattern(error_message: str, hours: int = 24) -> Dict:
    """
    ìœ ì‚¬í•œ ì—ëŸ¬ íŒ¨í„´ ë¶„ì„
    
    Args:
        error_message: ë¶„ì„í•  ì—ëŸ¬ ë©”ì‹œì§€
        hours: ë¶„ì„ ì‹œê°„ ë²”ìœ„ (ì‹œê°„)
        
    Returns:
        ì—ëŸ¬ íŒ¨í„´ ë¶„ì„ ê²°ê³¼
    """
    try:
        analyzer = MCPLogAnalyzer()
        analysis = analyzer.analyze_error_pattern(error_message, hours)
        
        if 'error' in analysis:
            return {
                'success': False,
                'error_message': error_message,
                'error': analysis['error']
            }
            
        return {
            'success': True,
            'analysis': analysis
        }
        
    except Exception as e:
        return {
            'success': False,
            'error_message': error_message,
            'error': str(e)
        }


def debug_session(since: str = "5m") -> Dict:
    """
    ë””ë²„ê¹…ìš© ì¢…í•© ë¡œê·¸ ìˆ˜ì§‘ ë° ë¶„ì„
    
    Args:
        since: ì¡°íšŒ ì‹œê°„ ë²”ìœ„ (ì˜ˆ: "5m", "1h", "30m")
        
    Returns:
        ë””ë²„ê¹…ì— í•„ìš”í•œ ì¢…í•© ì •ë³´
    """
    try:
        analyzer = MCPLogAnalyzer()
        debug_info = analyzer.debug_session(since)
        
        if 'error' in debug_info:
            return {
                'success': False,
                'since': since,
                'error': debug_info['error']
            }
            
        # í•µì‹¬ ì´ìŠˆ ìš”ì•½
        summary = debug_info.get('summary', {})
        critical_issues = []
        
        if summary.get('error_count', 0) > 0:
            critical_issues.append(f"ğŸš¨ {summary['error_count']}ê°œ ì—ëŸ¬ ë°œìƒ")
            
        if summary.get('slow_query_count', 0) > 0:
            critical_issues.append(f"ğŸŒ {summary['slow_query_count']}ê°œ ìŠ¬ë¡œìš° ì¿¼ë¦¬")
            
        avg_response = summary.get('avg_response_time_ms', 0)
        if avg_response > 1000:
            critical_issues.append(f"â±ï¸ í‰ê·  ì‘ë‹µì‹œê°„ {avg_response:.0f}ms")
            
        debug_info['critical_issues'] = critical_issues
        debug_info['health_status'] = 'critical' if len(critical_issues) > 2 else 'warning' if critical_issues else 'healthy'
        
        return {
            'success': True,
            'debug_info': debug_info
        }
        
    except Exception as e:
        return {
            'success': False,
            'since': since,
            'error': str(e)
        }


def get_performance_insights(hours: int = 1) -> Dict:
    """
    ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ë¶„ì„
    
    Args:
        hours: ë¶„ì„ ì‹œê°„ ë²”ìœ„ (ì‹œê°„)
        
    Returns:
        ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­
    """
    try:
        analyzer = MCPLogAnalyzer()
        insights = analyzer.get_performance_insights(hours)
        
        if 'error' in insights:
            return {
                'success': False,
                'hours': hours,
                'error': insights['error']
            }
            
        # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
        performance_score = 100
        
        # HTTP ì„±ëŠ¥ ì ìˆ˜
        if 'http' in insights.get('trends', {}):
            http_trends = insights['trends']['http']
            avg_response = http_trends.get('avg_response_time_ms', 0)
            
            if avg_response > 2000:
                performance_score -= 40
            elif avg_response > 1000:
                performance_score -= 20
            elif avg_response > 500:
                performance_score -= 10
                
        # DB ì„±ëŠ¥ ì ìˆ˜
        if 'database' in insights.get('trends', {}):
            db_trends = insights['trends']['database']
            avg_query = db_trends.get('avg_query_time_ms', 0)
            
            if avg_query > 1000:
                performance_score -= 30
            elif avg_query > 500:
                performance_score -= 15
            elif avg_query > 200:
                performance_score -= 5
                
        insights['performance_score'] = max(0, min(100, performance_score))
        insights['performance_grade'] = (
            'A' if performance_score >= 90 else
            'B' if performance_score >= 80 else
            'C' if performance_score >= 70 else
            'D' if performance_score >= 60 else 'F'
        )
        
        return {
            'success': True,
            'insights': insights
        }
        
    except Exception as e:
        return {
            'success': False,
            'hours': hours,
            'error': str(e)
        }


def get_system_health() -> Dict:
    """
    ì‹œìŠ¤í…œ ì „ë°˜ì ì¸ ê±´ê°•ë„ ì¡°íšŒ
    
    Returns:
        ì‹œìŠ¤í…œ ê±´ê°•ë„ ì ìˆ˜ ë° ìƒíƒœ
    """
    try:
        analyzer = MCPLogAnalyzer()
        
        # ìµœê·¼ 1ì‹œê°„ í†µê³„
        stats = analyzer.get_log_statistics("1h")
        
        if 'error' in stats:
            return {
                'success': False,
                'error': stats['error']
            }
            
        health_score = stats.get('health_score', 50)
        health_status = (
            'excellent' if health_score >= 95 else
            'good' if health_score >= 85 else
            'fair' if health_score >= 70 else
            'poor' if health_score >= 50 else 'critical'
        )
        
        # ì£¼ìš” ì§€í‘œ
        basic = stats.get('basic', {})
        by_level = stats.get('by_level', {})
        
        key_metrics = {
            'total_logs': basic.get('total_logs', 0),
            'error_count': by_level.get('ERROR', 0) + by_level.get('FATAL', 0),
            'warning_count': by_level.get('WARN', 0) + by_level.get('WARNING', 0),
            'error_rate': (by_level.get('ERROR', 0) + by_level.get('FATAL', 0)) / max(basic.get('total_logs', 1), 1),
            'health_score': health_score,
            'health_status': health_status
        }
        
        # ì•Œë¦¼ ë° ê¶Œì¥ì‚¬í•­
        alerts = stats.get('alerts', [])
        recommendations = []
        
        if key_metrics['error_rate'] > 0.1:
            recommendations.append("ì—ëŸ¬ìœ¨ì´ 10%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ê¸´ê¸‰ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        elif key_metrics['error_rate'] > 0.05:
            recommendations.append("ì—ëŸ¬ìœ¨ì´ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤. ëª¨ë‹ˆí„°ë§ì„ ê°•í™”í•˜ì„¸ìš”.")
            
        if key_metrics['total_logs'] == 0:
            recommendations.append("ë¡œê·¸ ìˆ˜ì§‘ì´ ì¤‘ë‹¨ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìˆ˜ì§‘ê¸° ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
        return {
            'success': True,
            'health': {
                'score': health_score,
                'status': health_status,
                'metrics': key_metrics,
                'alerts': alerts,
                'recommendations': recommendations,
                'timestamp': datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }


def search_logs(query: str, sources: List[str] = None, levels: List[str] = None, 
                since: str = "1h", limit: int = 50) -> Dict:
    """
    ê³ ê¸‰ ë¡œê·¸ ê²€ìƒ‰
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        sources: ì†ŒìŠ¤ í•„í„° (ì˜µì…˜)
        levels: ë ˆë²¨ í•„í„° (ì˜µì…˜)
        since: ì‹œê°„ ë²”ìœ„ (ì˜ˆ: "1h", "30m")
        limit: ê²°ê³¼ ê°œìˆ˜ ì œí•œ
        
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë° ë¶„ì„
    """
    try:
        analyzer = MCPLogAnalyzer()
        
        filters = {
            'limit': limit,
            'since': since
        }
        
        if sources:
            filters['sources'] = sources
        if levels:
            filters['levels'] = levels
            
        results = analyzer.search_logs_advanced(query, **filters)
        
        if 'error' in results:
            return {
                'success': False,
                'query': query,
                'error': results['error']
            }
            
        return {
            'success': True,
            'search_results': results
        }
        
    except Exception as e:
        return {
            'success': False,
            'query': query,
            'error': str(e)
        }


def get_log_summary(timerange: str = "1h") -> Dict:
    """
    ë¡œê·¸ ìš”ì•½ ì •ë³´ ì¡°íšŒ
    
    Args:
        timerange: ì‹œê°„ ë²”ìœ„ (ì˜ˆ: "1h", "6h", "24h")
        
    Returns:
        ë¡œê·¸ ìš”ì•½ í†µê³„
    """
    try:
        analyzer = MCPLogAnalyzer()
        stats = analyzer.get_log_statistics(timerange)
        
        if 'error' in stats:
            return {
                'success': False,
                'timerange': timerange,
                'error': stats['error']
            }
            
        # ìš”ì•½ ì •ë³´ êµ¬ì„±
        basic = stats.get('basic', {})
        by_source = stats.get('by_source_level', [])
        by_level = stats.get('by_level', {})
        
        summary = {
            'timerange': timerange,
            'total_logs': basic.get('total_logs', 0),
            'unique_sources': basic.get('unique_sources', 0),
            'error_rate': (by_level.get('ERROR', 0) + by_level.get('FATAL', 0)) / max(basic.get('total_logs', 1), 1),
            'top_sources': sorted(by_source, key=lambda x: x['count'], reverse=True)[:5],
            'level_distribution': dict(by_level),
            'health_score': stats.get('health_score', 50),
            'trends': stats.get('trends_analysis', {}),
            'timestamp': datetime.now().isoformat()
        }
        
        return {
            'success': True,
            'summary': summary
        }
        
    except Exception as e:
        return {
            'success': False,
            'timerange': timerange,
            'error': str(e)
        }


def analyze_incident(incident_description: str, timerange: str = "1h") -> Dict:
    """
    ì‚¬ê³  ë¶„ì„ - íŠ¹ì • ë¬¸ì œ ìƒí™©ì— ëŒ€í•œ ì¢…í•© ë¶„ì„
    
    Args:
        incident_description: ì‚¬ê³  ì„¤ëª…
        timerange: ë¶„ì„ ì‹œê°„ ë²”ìœ„
        
    Returns:
        ì‚¬ê³  ë¶„ì„ ê²°ê³¼ ë° ê·¼ë³¸ ì›ì¸ ë¶„ì„
    """
    try:
        analyzer = MCPLogAnalyzer()
        
        # 1. í‚¤ì›Œë“œ ê¸°ë°˜ ë¡œê·¸ ê²€ìƒ‰
        keywords = incident_description.lower().split()
        search_results = []
        
        for keyword in keywords:
            if len(keyword) > 3:  # 3ê¸€ì ì´ìƒ í‚¤ì›Œë“œë§Œ
                result = analyzer.search_logs_advanced(keyword, since=timerange, limit=20)
                if 'logs' in result:
                    search_results.extend(result['logs'])
                    
        # ì¤‘ë³µ ì œê±°
        unique_logs = {log['id']: log for log in search_results}.values()
        relevant_logs = list(unique_logs)
        
        # 2. ì—ëŸ¬ ë¡œê·¸ ìˆ˜ì§‘
        errors = analyzer.get_recent_errors(analyzer._parse_minutes(timerange))
        
        # 3. ì„±ëŠ¥ ì´ìŠˆ ë¶„ì„
        performance = analyzer.get_performance_insights(analyzer._parse_minutes(timerange) // 60 or 1)
        
        # 4. ì‹œìŠ¤í…œ ìƒíƒœ
        system_stats = analyzer.get_log_statistics(timerange)
        
        # 5. ê·¼ë³¸ ì›ì¸ ë¶„ì„
        root_cause_analysis = {
            'potential_causes': [],
            'evidence': [],
            'timeline': [],
            'affected_components': set(),
            'recommendations': []
        }
        
        # ì—ëŸ¬ íŒ¨í„´ ë¶„ì„
        if errors:
            error_sources = Counter([e.get('source', 'unknown') for e in errors])
            most_affected = error_sources.most_common(1)
            if most_affected:
                root_cause_analysis['affected_components'].add(most_affected[0][0])
                root_cause_analysis['potential_causes'].append(f"{most_affected[0][0]}ì—ì„œ {most_affected[0][1]}íšŒ ì—ëŸ¬ ë°œìƒ")
                
        # ì„±ëŠ¥ ì´ìŠˆ ë¶„ì„
        if 'error' not in performance:
            perf_issues = performance.get('performance_issues', [])
            for issue in perf_issues:
                root_cause_analysis['potential_causes'].append(issue['description'])
                root_cause_analysis['affected_components'].add('performance')
                
        # íƒ€ì„ë¼ì¸ êµ¬ì„±
        all_logs = relevant_logs + errors
        all_logs.sort(key=lambda x: x.get('timestamp', ''))
        
        for log in all_logs[-20:]:  # ìµœê·¼ 20ê°œ
            root_cause_analysis['timeline'].append({
                'timestamp': log.get('timestamp'),
                'source': log.get('source'),
                'level': log.get('level'),
                'message': log.get('message', '')[:100]  # ì²˜ìŒ 100ì
            })
            
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        if len(errors) > 5:
            root_cause_analysis['recommendations'].append("ë‹¤ìˆ˜ì˜ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ ìƒì„¸ ë¶„ì„í•˜ì—¬ ê³µí†µ ì›ì¸ì„ ì°¾ìœ¼ì„¸ìš”.")
            
        if 'performance' in root_cause_analysis['affected_components']:
            root_cause_analysis['recommendations'].append("ì„±ëŠ¥ ì´ìŠˆê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ê³¼ ë³‘ëª© ì§€ì ì„ í™•ì¸í•˜ì„¸ìš”.")
            
        if not root_cause_analysis['potential_causes']:
            root_cause_analysis['recommendations'].append("ëª…í™•í•œ ì›ì¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë” ë„“ì€ ì‹œê°„ ë²”ìœ„ì—ì„œ ë¶„ì„í•˜ê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.")
            
        root_cause_analysis['affected_components'] = list(root_cause_analysis['affected_components'])
        
        return {
            'success': True,
            'incident_analysis': {
                'description': incident_description,
                'timerange': timerange,
                'relevant_logs_count': len(relevant_logs),
                'error_count': len(errors),
                'root_cause_analysis': root_cause_analysis,
                'system_health': system_stats.get('health_score', 50),
                'timestamp': datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        return {
            'success': False,
            'incident_description': incident_description,
            'error': str(e)
        }


# MCP ì„œë²„ ë“±ë¡ìš© ë„êµ¬ ëª©ë¡
MCP_TOOLS = [
    {
        'name': 'get_recent_errors',
        'description': 'ìµœê·¼ ë°œìƒí•œ ì—ëŸ¬ ë¡œê·¸ë¥¼ ì¡°íšŒí•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤',
        'function': get_recent_errors,
        'parameters': {
            'minutes': {'type': 'integer', 'description': 'ì¡°íšŒí•  ì‹œê°„ ë²”ìœ„ (ë¶„)', 'default': 30}
        }
    },
    {
        'name': 'find_slow_queries',
        'description': 'ëŠë¦° ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ë¥¼ ì°¾ì•„ ë¶„ì„í•©ë‹ˆë‹¤',
        'function': find_slow_queries,
        'parameters': {
            'threshold_ms': {'type': 'integer', 'description': 'ëŠë¦° ì¿¼ë¦¬ ê¸°ì¤€ ì‹œê°„ (ë°€ë¦¬ì´ˆ)', 'default': 1000}
        }
    },
    {
        'name': 'trace_request',
        'description': 'íŠ¹ì • íŠ¸ë ˆì´ìŠ¤ IDì˜ ìš”ì²­ì„ ì¶”ì í•˜ì—¬ ì „ì²´ íë¦„ì„ ë¶„ì„í•©ë‹ˆë‹¤',
        'function': trace_request,
        'parameters': {
            'trace_id': {'type': 'string', 'description': 'ì¶”ì í•  íŠ¸ë ˆì´ìŠ¤ ID', 'required': True}
        }
    },
    {
        'name': 'analyze_error_pattern',
        'description': 'íŠ¹ì • ì—ëŸ¬ ë©”ì‹œì§€ì™€ ìœ ì‚¬í•œ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤',
        'function': analyze_error_pattern,
        'parameters': {
            'error_message': {'type': 'string', 'description': 'ë¶„ì„í•  ì—ëŸ¬ ë©”ì‹œì§€', 'required': True},
            'hours': {'type': 'integer', 'description': 'ë¶„ì„ ì‹œê°„ ë²”ìœ„ (ì‹œê°„)', 'default': 24}
        }
    },
    {
        'name': 'debug_session',
        'description': 'ë””ë²„ê¹…ì— í•„ìš”í•œ ì¢…í•©ì ì¸ ë¡œê·¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤',
        'function': debug_session,
        'parameters': {
            'since': {'type': 'string', 'description': 'ì¡°íšŒ ì‹œê°„ ë²”ìœ„ (ì˜ˆ: 5m, 1h)', 'default': '5m'}
        }
    },
    {
        'name': 'get_performance_insights',
        'description': 'ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ë¶„ì„í•˜ê³  ë³‘ëª©ì ì„ ì°¾ìŠµë‹ˆë‹¤',
        'function': get_performance_insights,
        'parameters': {
            'hours': {'type': 'integer', 'description': 'ë¶„ì„ ì‹œê°„ ë²”ìœ„ (ì‹œê°„)', 'default': 1}
        }
    },
    {
        'name': 'get_system_health',
        'description': 'ì‹œìŠ¤í…œ ì „ë°˜ì ì¸ ê±´ê°•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤',
        'function': get_system_health,
        'parameters': {}
    },
    {
        'name': 'search_logs',
        'description': 'ê³ ê¸‰ ê²€ìƒ‰ìœ¼ë¡œ íŠ¹ì • ë¡œê·¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤',
        'function': search_logs,
        'parameters': {
            'query': {'type': 'string', 'description': 'ê²€ìƒ‰ ì¿¼ë¦¬', 'required': True},
            'sources': {'type': 'array', 'description': 'ì†ŒìŠ¤ í•„í„° (ì˜µì…˜)'},
            'levels': {'type': 'array', 'description': 'ë ˆë²¨ í•„í„° (ì˜µì…˜)'},
            'since': {'type': 'string', 'description': 'ì‹œê°„ ë²”ìœ„', 'default': '1h'},
            'limit': {'type': 'integer', 'description': 'ê²°ê³¼ ê°œìˆ˜ ì œí•œ', 'default': 50}
        }
    },
    {
        'name': 'get_log_summary',
        'description': 'íŠ¹ì • ì‹œê°„ ë²”ìœ„ì˜ ë¡œê·¸ ìš”ì•½ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤',
        'function': get_log_summary,
        'parameters': {
            'timerange': {'type': 'string', 'description': 'ì‹œê°„ ë²”ìœ„ (ì˜ˆ: 1h, 6h, 24h)', 'default': '1h'}
        }
    },
    {
        'name': 'analyze_incident',
        'description': 'ì‚¬ê³  ìƒí™©ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ê·¼ë³¸ ì›ì¸ì„ ì°¾ìŠµë‹ˆë‹¤',
        'function': analyze_incident,
        'parameters': {
            'incident_description': {'type': 'string', 'description': 'ì‚¬ê³  ì„¤ëª…', 'required': True},
            'timerange': {'type': 'string', 'description': 'ë¶„ì„ ì‹œê°„ ë²”ìœ„', 'default': '1h'}
        }
    }
]


class MCPServer:
    """MCP ì„œë²„ ë˜í¼"""
    
    def __init__(self, config_path: str = None):
        self.analyzer = MCPLogAnalyzer(config_path)
        self.tools = {tool['name']: tool for tool in MCP_TOOLS}
        
    def handle_tool_call(self, tool_name: str, parameters: Dict) -> Dict:
        """MCP ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬"""
        if tool_name not in self.tools:
            return {
                'success': False,
                'error': f'ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {tool_name}',
                'available_tools': list(self.tools.keys())
            }
            
        tool = self.tools[tool_name]
        function = tool['function']
        
        try:
            result = function(**parameters)
            return result
        except Exception as e:
            return {
                'success': False,
                'error': f'ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}',
                'tool_name': tool_name,
                'parameters': parameters
            }
            
    def get_available_tools(self) -> List[Dict]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
        return [{
            'name': tool['name'],
            'description': tool['description'],
            'parameters': tool['parameters']
        } for tool in MCP_TOOLS]
        
    def generate_context_for_llm(self, query: str) -> Dict:
        """LLMì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ìë™ ìƒì„±"""
        context = {
            'query': query,
            'timestamp': datetime.now().isoformat(),
            'suggested_tools': [],
            'relevant_data': {}
        }
        
        query_lower = query.lower()
        
        # ì¿¼ë¦¬ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ ì¶”ì²œ
        if any(word in query_lower for word in ['error', 'ì—ëŸ¬', 'ì˜¤ë¥˜', 'exception']):
            context['suggested_tools'].append('get_recent_errors')
            context['suggested_tools'].append('analyze_error_pattern')
            
        if any(word in query_lower for word in ['slow', 'ëŠë¦°', 'performance', 'ì„±ëŠ¥']):
            context['suggested_tools'].append('find_slow_queries')
            context['suggested_tools'].append('get_performance_insights')
            
        if any(word in query_lower for word in ['trace', 'ì¶”ì ', 'request', 'ìš”ì²­']):
            context['suggested_tools'].append('trace_request')
            
        if any(word in query_lower for word in ['debug', 'ë””ë²„ê·¸', 'troubleshoot', 'ë¬¸ì œí•´ê²°']):
            context['suggested_tools'].append('debug_session')
            
        if any(word in query_lower for word in ['health', 'ê±´ê°•', 'status', 'ìƒíƒœ']):
            context['suggested_tools'].append('get_system_health')
            context['suggested_tools'].append('get_log_summary')
            
        if any(word in query_lower for word in ['incident', 'ì‚¬ê³ ', 'outage', 'ì¥ì• ']):
            context['suggested_tools'].append('analyze_incident')
            
        if any(word in query_lower for word in ['search', 'ê²€ìƒ‰', 'find', 'ì°¾ê¸°']):
            context['suggested_tools'].append('search_logs')
            
        # ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘
        try:
            # ìµœê·¼ ì‹œìŠ¤í…œ ìƒíƒœ
            health = get_system_health()
            if health['success']:
                context['relevant_data']['system_health'] = health['health']
                
            # ìµœê·¼ ì—ëŸ¬ ìš”ì•½
            recent_errors = get_recent_errors(10)  # ìµœê·¼ 10ë¶„
            if recent_errors['success'] and recent_errors['count'] > 0:
                context['relevant_data']['recent_errors'] = {
                    'count': recent_errors['count'],
                    'most_recent': recent_errors['errors'][0] if recent_errors['errors'] else None
                }
                
        except Exception as e:
            context['relevant_data']['context_error'] = str(e)
            
        return context


def create_mcp_server(config_path: str = None) -> MCPServer:
    """MCP ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return MCPServer(config_path)


def get_mcp_tools_schema() -> Dict:
    """MCP ë„êµ¬ ìŠ¤í‚¤ë§ˆ ë°˜í™˜ (MCP í´ë¼ì´ì–¸íŠ¸ìš©)"""
    return {
        'tools': [
            {
                'name': tool['name'],
                'description': tool['description'],
                'inputSchema': {
                    'type': 'object',
                    'properties': tool['parameters'],
                    'required': [
                        param_name for param_name, param_info in tool['parameters'].items()
                        if param_info.get('required', False)
                    ]
                }
            }
            for tool in MCP_TOOLS
        ]
    }


# CLI ì¸í„°í˜ì´ìŠ¤
def main():
    """CLI ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='MCP ë¡œê·¸ ë¶„ì„ ë„êµ¬')
    parser.add_argument('--config', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--tool', help='ì‹¤í–‰í•  ë„êµ¬ ì´ë¦„')
    parser.add_argument('--params', help='ë„êµ¬ íŒŒë¼ë¯¸í„° (JSON í˜•ì‹)')
    parser.add_argument('--list-tools', action='store_true', help='ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡')
    parser.add_argument('--test', action='store_true', help='ëª¨ë“  ë„êµ¬ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--context', help='LLM ì»¨í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ ì¿¼ë¦¬')
    
    args = parser.parse_args()
    
    server = create_mcp_server(args.config)
    
    if args.list_tools:
        print("=== ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ë“¤ ===")
        tools = server.get_available_tools()
        for tool in tools:
            print(f"\nğŸ“Š {tool['name']}")
            print(f"   ì„¤ëª…: {tool['description']}")
            if tool['parameters']:
                print("   íŒŒë¼ë¯¸í„°:")
                for param_name, param_info in tool['parameters'].items():
                    required = " (í•„ìˆ˜)" if param_info.get('required') else ""
                    default = f" [ê¸°ë³¸ê°’: {param_info.get('default')}]" if 'default' in param_info else ""
                    print(f"     - {param_name}: {param_info.get('description', '')}{required}{default}")
                    
    elif args.context:
        print("=== LLM ì»¨í…ìŠ¤íŠ¸ ìƒì„± ===")
        context = server.generate_context_for_llm(args.context)
        print(json.dumps(context, indent=2, ensure_ascii=False))
        
    elif args.test:
        print("=== MCP ë„êµ¬ í…ŒìŠ¤íŠ¸ ===")
        
        # 1. ì‹œìŠ¤í…œ ê±´ê°•ë„
        print("\n1. ì‹œìŠ¤í…œ ê±´ê°•ë„ í™•ì¸...")
        result = server.handle_tool_call('get_system_health', {})
        if result['success']:
            health = result['health']
            print(f"   ìƒíƒœ: {health['status']} (ì ìˆ˜: {health['score']})")
            print(f"   ì´ ë¡œê·¸: {health['metrics']['total_logs']}ê°œ")
            print(f"   ì—ëŸ¬ìœ¨: {health['metrics']['error_rate']*100:.1f}%")
        else:
            print(f"   ì‹¤íŒ¨: {result.get('error')}")
            
        # 2. ìµœê·¼ ì—ëŸ¬
        print("\n2. ìµœê·¼ ì—ëŸ¬ í™•ì¸...")
        result = server.handle_tool_call('get_recent_errors', {'minutes': 30})
        if result['success']:
            print(f"   ìµœê·¼ 30ë¶„ê°„ ì—ëŸ¬: {result['count']}ê°œ")
            if result['count'] > 0:
                print(f"   ì£¼ìš” ì†ŒìŠ¤: {list(result['analysis']['error_by_source'].keys())}")
        else:
            print(f"   ì‹¤íŒ¨: {result.get('error')}")
            
        # 3. ìŠ¬ë¡œìš° ì¿¼ë¦¬
        print("\n3. ìŠ¬ë¡œìš° ì¿¼ë¦¬ í™•ì¸...")
        result = server.handle_tool_call('find_slow_queries', {'threshold_ms': 500})
        if result['success']:
            print(f"   ìŠ¬ë¡œìš° ì¿¼ë¦¬: {result['count']}ê°œ")
            if result['count'] > 0:
                slowest = result['analysis']['slowest_query_ms']
                print(f"   ê°€ì¥ ëŠë¦° ì¿¼ë¦¬: {slowest}ms")
        else:
            print(f"   ì‹¤íŒ¨: {result.get('error')}")
            
        # 4. ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸
        print("\n4. ì„±ëŠ¥ ë¶„ì„...")
        result = server.handle_tool_call('get_performance_insights', {'hours': 1})
        if result['success']:
            insights = result['insights']
            score = insights.get('performance_score', 0)
            grade = insights.get('performance_grade', 'N/A')
            print(f"   ì„±ëŠ¥ ì ìˆ˜: {score} (ë“±ê¸‰: {grade})")
            
            if 'trends' in insights and 'http' in insights['trends']:
                avg_response = insights['trends']['http']['avg_response_time_ms']
                print(f"   í‰ê·  ì‘ë‹µì‹œê°„: {avg_response:.0f}ms")
        else:
            print(f"   ì‹¤íŒ¨: {result.get('error')}")
            
        # 5. ë¡œê·¸ ìš”ì•½
        print("\n5. ë¡œê·¸ ìš”ì•½...")
        result = server.handle_tool_call('get_log_summary', {'timerange': '1h'})
        if result['success']:
            summary = result['summary']
            print(f"   ìµœê·¼ 1ì‹œê°„ ë¡œê·¸: {summary['total_logs']}ê°œ")
            print(f"   ìœ ë‹ˆí¬ ì†ŒìŠ¤: {summary['unique_sources']}ê°œ")
            print(f"   ì—ëŸ¬ìœ¨: {summary['error_rate']*100:.1f}%")
        else:
            print(f"   ì‹¤íŒ¨: {result.get('error')}")
            
    elif args.tool:
        if not args.params:
            print("--params ì˜µì…˜ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.")
            print('ì˜ˆ: --params \'{"minutes": 30}\'')
            return
            
        try:
            params = json.loads(args.params)
            result = server.handle_tool_call(args.tool, params)
            print(json.dumps(result, indent=2, ensure_ascii=False))
        except json.JSONDecodeError as e:
            print(f"JSON íŒŒë¼ë¯¸í„° íŒŒì‹± ì˜¤ë¥˜: {e}")
        except Exception as e:
            print(f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            
    else:
        parser.print_help()


if __name__ == '__main__':
    main()


# ì‚¬ìš© ì˜ˆì‹œ
"""
CLI ì‚¬ìš©ë²•:

# ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡
python mcp_tools.py --list-tools

# ì‹œìŠ¤í…œ ê±´ê°•ë„ í™•ì¸
python mcp_tools.py --tool get_system_health --params '{}'

# ìµœê·¼ 30ë¶„ ì—ëŸ¬ ì¡°íšŒ
python mcp_tools.py --tool get_recent_errors --params '{"minutes": 30}'

# ìŠ¬ë¡œìš° ì¿¼ë¦¬ ì°¾ê¸°
python mcp_tools.py --tool find_slow_queries --params '{"threshold_ms": 1000}'

# ë¡œê·¸ ê²€ìƒ‰
python mcp_tools.py --tool search_logs --params '{"query": "database timeout", "since": "1h"}'

# ì‚¬ê³  ë¶„ì„
python mcp_tools.py --tool analyze_incident --params '{"incident_description": "API ì‘ë‹µ ì§€ì—°", "timerange": "2h"}'

# ëª¨ë“  ë„êµ¬ í…ŒìŠ¤íŠ¸
python mcp_tools.py --test

# LLM ì»¨í…ìŠ¤íŠ¸ ìƒì„±
python mcp_tools.py --context "ìµœê·¼ì— ë°ì´í„°ë² ì´ìŠ¤ ì—ëŸ¬ê°€ ë§ì´ ë°œìƒí•˜ëŠ”ë° ì›ì¸ì„ ì°¾ì•„ì¤˜"

Python API ì‚¬ìš©ë²•:

from mcp_tools import create_mcp_server

# MCP ì„œë²„ ìƒì„±
server = create_mcp_server("./config.yaml")

# ë„êµ¬ ì‹¤í–‰
result = server.handle_tool_call('get_recent_errors', {'minutes': 30})
print(result)

# LLM ì»¨í…ìŠ¤íŠ¸ ìƒì„±
context = server.generate_context_for_llm("ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¬¸ì œë¥¼ ë¶„ì„í•´ì¤˜")
print(context)

# ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡
tools = server.get_available_tools()
for tool in tools:
    print(f"{tool['name']}: {tool['description']}")
"""