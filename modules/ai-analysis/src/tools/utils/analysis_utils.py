#!/usr/bin/env python3
"""
Enhanced Analysis Utilities
ê³ ë„í™”ëœ í”„ë¡œì íŠ¸ ë¶„ì„ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë„êµ¬ë“¤

ì£¼ìš” ê°œì„ ì‚¬í•­:
- íƒ€ì… ì•ˆì •ì„± ê°•í™” (Pydantic ëª¨ë¸)
- ì˜ì¡´ì„± ê·¸ë˜í”„ ë¶„ì„
- ë³´ì•ˆ ë¶„ì„ ì¶”ê°€
- ì„±ëŠ¥ ìµœì í™” (ìºì‹±, ë³‘ë ¬ì²˜ë¦¬)
- ì½”ë“œ í’ˆì§ˆ ë©”íŠ¸ë¦­ í™•ì¥
- AI ê¸°ë°˜ ë¶„ì„ ì¤€ë¹„
"""

import ast
import re
import os
import hashlib
import networkx as nx
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Union, Set, Tuple, Generator
from dataclasses import dataclass, field
from pydantic import BaseModel, Field, validator
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from enum import Enum
import json

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class SecurityLevel(str, Enum):
    """ë³´ì•ˆ ìœ„í—˜ ìˆ˜ì¤€"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


class LanguageType(str, Enum):
    """ì§€ì›í•˜ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´"""
    PYTHON = "Python"
    JAVASCRIPT = "JavaScript"
    TYPESCRIPT = "TypeScript"
    REACT_JSX = "React JSX"
    REACT_TSX = "React TSX"
    JAVA = "Java"
    CPP = "C++"
    C = "C"
    CSHARP = "C#"
    GO = "Go"
    RUST = "Rust"
    PHP = "PHP"
    RUBY = "Ruby"
    SWIFT = "Swift"
    KOTLIN = "Kotlin"
    UNKNOWN = "Unknown"


class SecurityIssue(BaseModel):
    """ë³´ì•ˆ ì´ìŠˆ ëª¨ë¸"""
    type: str = Field(..., description="ë³´ì•ˆ ì´ìŠˆ ìœ í˜•")
    severity: SecurityLevel = Field(..., description="ì‹¬ê°ë„")
    line: int = Field(..., description="ë¼ì¸ ë²ˆí˜¸")
    description: str = Field(..., description="ì´ìŠˆ ì„¤ëª…")
    recommendation: str = Field(..., description="ê¶Œì¥ì‚¬í•­")
    code_snippet: Optional[str] = Field(None, description="ë¬¸ì œ ì½”ë“œ ì¡°ê°")


class DependencyInfo(BaseModel):
    """ì˜ì¡´ì„± ì •ë³´ ëª¨ë¸"""
    name: str = Field(..., description="ì˜ì¡´ì„± ì´ë¦„")
    version: Optional[str] = Field(None, description="ë²„ì „")
    import_type: str = Field(..., description="import íƒ€ì…")
    module: str = Field(..., description="ëª¨ë“ˆëª…")
    line: int = Field(..., description="ë¼ì¸ ë²ˆí˜¸")
    is_external: bool = Field(True, description="ì™¸ë¶€ ì˜ì¡´ì„± ì—¬ë¶€")
    is_standard_library: bool = Field(False, description="í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—¬ë¶€")


class FunctionMetrics(BaseModel):
    """í•¨ìˆ˜ ë©”íŠ¸ë¦­ ëª¨ë¸"""
    name: str = Field(..., description="í•¨ìˆ˜ëª…")
    line_start: int = Field(..., description="ì‹œì‘ ë¼ì¸")
    line_end: int = Field(..., description="ë ë¼ì¸")
    cyclomatic_complexity: int = Field(0, description="ìˆœí™˜ ë³µì¡ë„")
    cognitive_complexity: int = Field(0, description="ì¸ì§€ ë³µì¡ë„")
    halstead_complexity: Dict[str, float] = Field(default_factory=dict, description="í• ìŠ¤í…Œë“œ ë³µì¡ë„")
    parameters_count: int = Field(0, description="ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜")
    return_statements: int = Field(0, description="return ë¬¸ ê°œìˆ˜")
    is_async: bool = Field(False, description="ë¹„ë™ê¸° í•¨ìˆ˜ ì—¬ë¶€")
    is_recursive: bool = Field(False, description="ì¬ê·€ í•¨ìˆ˜ ì—¬ë¶€")
    docstring_quality: int = Field(0, description="ë…ìŠ¤íŠ¸ë§ í’ˆì§ˆ ì ìˆ˜ (0-100)")


class ClassMetrics(BaseModel):
    """í´ë˜ìŠ¤ ë©”íŠ¸ë¦­ ëª¨ë¸"""
    name: str = Field(..., description="í´ë˜ìŠ¤ëª…")
    line_start: int = Field(..., description="ì‹œì‘ ë¼ì¸")
    line_end: int = Field(..., description="ë ë¼ì¸")
    methods_count: int = Field(0, description="ë©”ì„œë“œ ê°œìˆ˜")
    attributes_count: int = Field(0, description="ì†ì„± ê°œìˆ˜")
    inheritance_depth: int = Field(0, description="ìƒì† ê¹Šì´")
    coupling_factor: float = Field(0.0, description="ê²°í•©ë„")
    cohesion_factor: float = Field(0.0, description="ì‘ì§‘ë„")
    is_abstract: bool = Field(False, description="ì¶”ìƒ í´ë˜ìŠ¤ ì—¬ë¶€")
    design_patterns: List[str] = Field(default_factory=list, description="íƒì§€ëœ ë””ìì¸ íŒ¨í„´")


class CodeQualityMetrics(BaseModel):
    """ì½”ë“œ í’ˆì§ˆ ë©”íŠ¸ë¦­"""
    maintainability_index: float = Field(0.0, description="ìœ ì§€ë³´ìˆ˜ì„± ì§€ìˆ˜")
    technical_debt_ratio: float = Field(0.0, description="ê¸°ìˆ  ë¶€ì±„ ë¹„ìœ¨")
    code_duplication_ratio: float = Field(0.0, description="ì½”ë“œ ì¤‘ë³µ ë¹„ìœ¨")
    test_coverage_estimate: float = Field(0.0, description="í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ì¶”ì •")
    documentation_ratio: float = Field(0.0, description="ë¬¸ì„œí™” ë¹„ìœ¨")
    complexity_distribution: Dict[str, int] = Field(default_factory=dict, description="ë³µì¡ë„ ë¶„í¬")


class AnalysisResult(BaseModel):
    """ë¶„ì„ ê²°ê³¼ ëª¨ë¸"""
    file_path: str = Field(..., description="íŒŒì¼ ê²½ë¡œ")
    file_hash: str = Field(..., description="íŒŒì¼ í•´ì‹œ")
    language: LanguageType = Field(..., description="í”„ë¡œê·¸ë˜ë° ì–¸ì–´")
    lines_of_code: int = Field(0, description="ì½”ë“œ ë¼ì¸ ìˆ˜")
    functions: List[FunctionMetrics] = Field(default_factory=list, description="í•¨ìˆ˜ ë©”íŠ¸ë¦­")
    classes: List[ClassMetrics] = Field(default_factory=list, description="í´ë˜ìŠ¤ ë©”íŠ¸ë¦­")
    dependencies: List[DependencyInfo] = Field(default_factory=list, description="ì˜ì¡´ì„± ì •ë³´")
    security_issues: List[SecurityIssue] = Field(default_factory=list, description="ë³´ì•ˆ ì´ìŠˆ")
    env_vars: List[Dict[str, Any]] = Field(default_factory=list, description="í™˜ê²½ë³€ìˆ˜")
    quality_metrics: CodeQualityMetrics = Field(default_factory=CodeQualityMetrics, description="í’ˆì§ˆ ë©”íŠ¸ë¦­")
    analysis_timestamp: datetime = Field(default_factory=datetime.now, description="ë¶„ì„ ì‹œê°")
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }


class DependencyGraph(BaseModel):
    """ì˜ì¡´ì„± ê·¸ë˜í”„ ëª¨ë¸"""
    nodes: List[str] = Field(default_factory=list, description="ë…¸ë“œ ëª©ë¡")
    edges: List[Tuple[str, str]] = Field(default_factory=list, description="ì—£ì§€ ëª©ë¡")
    circular_dependencies: List[List[str]] = Field(default_factory=list, description="ìˆœí™˜ ì˜ì¡´ì„±")
    depth_analysis: Dict[str, int] = Field(default_factory=dict, description="ê¹Šì´ ë¶„ì„")
    centrality_metrics: Dict[str, float] = Field(default_factory=dict, description="ì¤‘ì‹¬ì„± ë©”íŠ¸ë¦­")


class ProjectSummary(BaseModel):
    """í”„ë¡œì íŠ¸ ìš”ì•½ ëª¨ë¸"""
    total_files: int = Field(0, description="ì´ íŒŒì¼ ìˆ˜")
    total_lines: int = Field(0, description="ì´ ë¼ì¸ ìˆ˜")
    language_distribution: Dict[str, int] = Field(default_factory=dict, description="ì–¸ì–´ë³„ ë¶„í¬")
    complexity_summary: Dict[str, float] = Field(default_factory=dict, description="ë³µì¡ë„ ìš”ì•½")
    security_summary: Dict[str, int] = Field(default_factory=dict, description="ë³´ì•ˆ ìš”ì•½")
    quality_summary: Dict[str, float] = Field(default_factory=dict, description="í’ˆì§ˆ ìš”ì•½")
    dependency_graph: Optional[DependencyGraph] = Field(None, description="ì˜ì¡´ì„± ê·¸ë˜í”„")
    hotspots: List[Dict[str, Any]] = Field(default_factory=list, description="í•«ìŠ¤íŒŸ ë¶„ì„")
    recommendations: List[str] = Field(default_factory=list, description="ê°œì„  ê¶Œì¥ì‚¬í•­")


class EnhancedProjectAnalyzer:
    """ê³ ë„í™”ëœ í”„ë¡œì íŠ¸ ë¶„ì„ê¸°"""
    
    def __init__(self, cache_enabled: bool = True, max_workers: int = 4):
        self.cache_enabled = cache_enabled
        self.max_workers = max_workers
        self.cache = {}
        
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì ë§¤í•‘
        self.language_mapping = {
            '.py': LanguageType.PYTHON,
            '.js': LanguageType.JAVASCRIPT,
            '.ts': LanguageType.TYPESCRIPT,
            '.jsx': LanguageType.REACT_JSX,
            '.tsx': LanguageType.REACT_TSX,
            '.java': LanguageType.JAVA,
            '.cpp': LanguageType.CPP,
            '.c': LanguageType.C,
            '.cs': LanguageType.CSHARP,
            '.go': LanguageType.GO,
            '.rs': LanguageType.RUST,
            '.php': LanguageType.PHP,
            '.rb': LanguageType.RUBY,
            '.swift': LanguageType.SWIFT,
            '.kt': LanguageType.KOTLIN,
        }
        
        # ë³´ì•ˆ íŒ¨í„´ ì •ì˜
        self.security_patterns = {
            'hardcoded_secrets': [
                (r'(?i)(password|pwd|secret|key|token)\s*=\s*["\'][^"\']+["\']', 
                 SecurityLevel.CRITICAL, "í•˜ë“œì½”ë”©ëœ ë³´ì•ˆ ì •ë³´"),
                (r'(?i)(api_key|apikey)\s*=\s*["\'][^"\']+["\']', 
                 SecurityLevel.CRITICAL, "í•˜ë“œì½”ë”©ëœ API í‚¤"),
            ],
            'sql_injection': [
                (r'(?i)(select|insert|update|delete).*\+.*', 
                 SecurityLevel.HIGH, "SQL ì¸ì ì…˜ ì·¨ì•½ì  ê°€ëŠ¥ì„±"),
                (r'(?i)execute\s*\(\s*["\'].*\+.*["\']', 
                 SecurityLevel.HIGH, "ë™ì  SQL ì‹¤í–‰"),
            ],
            'xss_vulnerability': [
                (r'(?i)innerHTML\s*=\s*.*\+', 
                 SecurityLevel.MEDIUM, "XSS ì·¨ì•½ì  ê°€ëŠ¥ì„±"),
                (r'(?i)document\.write\s*\(.*\+', 
                 SecurityLevel.MEDIUM, "ë™ì  DOM ì¡°ì‘"),
            ],
            'weak_crypto': [
                (r'(?i)(md5|sha1)\s*\(', 
                 SecurityLevel.MEDIUM, "ì•½í•œ ì•”í˜¸í™” ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©"),
            ]
        }
        
        # í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡ (Python)
        self.python_stdlib = {
            'os', 'sys', 'json', 're', 'time', 'datetime', 'collections',
            'itertools', 'functools', 'pathlib', 'typing', 'logging',
            'unittest', 'asyncio', 'threading', 'multiprocessing'
        }
        
        # ë””ìì¸ íŒ¨í„´ íƒì§€ íŒ¨í„´
        self.design_patterns = {
            'singleton': [r'class\s+\w+.*:\s*\n\s*_instance\s*=\s*None'],
            'factory': [r'def\s+create_\w+\s*\(', r'class\s+\w*Factory'],
            'observer': [r'def\s+(add|remove)_observer', r'def\s+notify'],
            'decorator': [r'@\w+\s*\n\s*def\s+\w+'],
            'builder': [r'class\s+\w*Builder', r'def\s+build\s*\('],
        }
    
    def analyze_file(self, file_path: Union[str, Path]) -> AnalysisResult:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
        file_path = Path(file_path)
        
        # ìºì‹œ í™•ì¸
        if self.cache_enabled:
            file_hash = self._get_file_hash(file_path)
            if file_hash in self.cache:
                logger.debug(f"ìºì‹œì—ì„œ ë¶„ì„ ê²°ê³¼ ë°˜í™˜: {file_path}")
                return self.cache[file_hash]
        
        try:
            # íŒŒì¼ ë‚´ìš© ì½ê¸°
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # ì–¸ì–´ íƒì§€
            language = self._detect_language(file_path)
            
            # ë¶„ì„ ìˆ˜í–‰
            result = AnalysisResult(
                file_path=str(file_path),
                file_hash=file_hash,
                language=language,
                lines_of_code=len([line for line in content.splitlines() if line.strip()])
            )
            
            # ì–¸ì–´ë³„ ë¶„ì„
            if language == LanguageType.PYTHON:
                result = self._analyze_python_file(file_path, content, result)
            elif language in [LanguageType.JAVASCRIPT, LanguageType.TYPESCRIPT, 
                             LanguageType.REACT_JSX, LanguageType.REACT_TSX]:
                result = self._analyze_javascript_file(file_path, content, result)
            else:
                result = self._analyze_generic_file(file_path, content, result)
            
            # ë³´ì•ˆ ë¶„ì„
            result.security_issues = self._analyze_security(content)
            
            # í™˜ê²½ë³€ìˆ˜ ë¶„ì„
            result.env_vars = self._extract_env_vars(content, language)
            
            # ì½”ë“œ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            result.quality_metrics = self._calculate_quality_metrics(result)
            
            # ìºì‹œ ì €ì¥
            if self.cache_enabled:
                self.cache[file_hash] = result
            
            return result
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {e}")
            return AnalysisResult(
                file_path=str(file_path),
                file_hash="",
                language=LanguageType.UNKNOWN,
                lines_of_code=0
            )
    
    def analyze_project(self, project_path: Union[str, Path], 
                       recursive: bool = True,
                       file_patterns: Optional[List[str]] = None) -> Tuple[List[AnalysisResult], ProjectSummary]:
        """í”„ë¡œì íŠ¸ ì „ì²´ ë¶„ì„"""
        project_path = Path(project_path)
        
        if file_patterns is None:
            file_patterns = list(self.language_mapping.keys())
        
        # ë¶„ì„í•  íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        files_to_analyze = []
        for pattern in file_patterns:
            if recursive:
                files_to_analyze.extend(project_path.rglob(f"*{pattern}"))
            else:
                files_to_analyze.extend(project_path.glob(f"*{pattern}"))
        
        # ìŠ¤í‚µí•  íŒŒì¼ í•„í„°ë§
        files_to_analyze = [f for f in files_to_analyze if not self._should_skip_file(f)]
        
        logger.info(f"ë¶„ì„ ëŒ€ìƒ íŒŒì¼: {len(files_to_analyze)}ê°œ")
        
        # ë³‘ë ¬ ë¶„ì„
        analysis_results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_file = {executor.submit(self.analyze_file, file_path): file_path 
                             for file_path in files_to_analyze}
            
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    result = future.result()
                    analysis_results.append(result)
                except Exception as e:
                    logger.error(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {e}")
        
        # í”„ë¡œì íŠ¸ ìš”ì•½ ìƒì„±
        summary = self._generate_project_summary(analysis_results)
        
        return analysis_results, summary
    
    def build_dependency_graph(self, analysis_results: List[AnalysisResult]) -> DependencyGraph:
        """ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶•"""
        graph = nx.DiGraph()
        
        # ë…¸ë“œì™€ ì—£ì§€ ì¶”ê°€
        for result in analysis_results:
            file_name = Path(result.file_path).stem
            graph.add_node(file_name)
            
            for dep in result.dependencies:
                if not dep.is_external:
                    graph.add_edge(file_name, dep.name)
        
        # ìˆœí™˜ ì˜ì¡´ì„± íƒì§€
        try:
            cycles = list(nx.simple_cycles(graph))
        except nx.NetworkXError:
            cycles = []
        
        # ì¤‘ì‹¬ì„± ë¶„ì„
        try:
            centrality = nx.betweenness_centrality(graph)
        except:
            centrality = {}
        
        # ê¹Šì´ ë¶„ì„
        depth_analysis = {}
        for node in graph.nodes():
            try:
                depth_analysis[node] = nx.shortest_path_length(graph, source=node)
            except:
                depth_analysis[node] = 0
        
        return DependencyGraph(
            nodes=list(graph.nodes()),
            edges=list(graph.edges()),
            circular_dependencies=cycles,
            depth_analysis=depth_analysis,
            centrality_metrics=centrality
        )
    
    def _analyze_python_file(self, file_path: Path, content: str, result: AnalysisResult) -> AnalysisResult:
        """Python íŒŒì¼ ë¶„ì„"""
        try:
            tree = ast.parse(content)
            
            # í•¨ìˆ˜ ë¶„ì„
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    func_metrics = self._analyze_python_function(node, content)
                    result.functions.append(func_metrics)
                
                elif isinstance(node, ast.ClassDef):
                    class_metrics = self._analyze_python_class(node, content)
                    result.classes.append(class_metrics)
                
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    deps = self._analyze_python_import(node)
                    result.dependencies.extend(deps)
            
        except SyntaxError as e:
            logger.warning(f"Python êµ¬ë¬¸ ë¶„ì„ ì‹¤íŒ¨: {file_path}, {e}")
            # ì •ê·œì‹ ê¸°ë°˜ í´ë°± ë¶„ì„
            result = self._analyze_with_regex(content, result)
        
        return result
    
    def _analyze_python_function(self, node: ast.FunctionDef, content: str) -> FunctionMetrics:
        """Python í•¨ìˆ˜ ë¶„ì„"""
        lines = content.splitlines()
        
        # ê¸°ë³¸ ì •ë³´
        func_name = node.name
        line_start = node.lineno
        line_end = getattr(node, 'end_lineno', line_start)
        
        # ë³µì¡ë„ ê³„ì‚°
        cyclomatic = self._calculate_cyclomatic_complexity(node)
        cognitive = self._calculate_cognitive_complexity(node)
        halstead = self._calculate_halstead_complexity(node, content)
        
        # ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜
        params_count = len(node.args.args)
        
        # return ë¬¸ ê°œìˆ˜
        return_count = len([n for n in ast.walk(node) if isinstance(n, ast.Return)])
        
        # ì¬ê·€ í•¨ìˆ˜ ì—¬ë¶€
        is_recursive = any(isinstance(n, ast.Call) and 
                          isinstance(n.func, ast.Name) and 
                          n.func.id == func_name 
                          for n in ast.walk(node))
        
        # ë…ìŠ¤íŠ¸ë§ í’ˆì§ˆ í‰ê°€
        docstring_quality = self._evaluate_docstring_quality(ast.get_docstring(node))
        
        return FunctionMetrics(
            name=func_name,
            line_start=line_start,
            line_end=line_end,
            cyclomatic_complexity=cyclomatic,
            cognitive_complexity=cognitive,
            halstead_complexity=halstead,
            parameters_count=params_count,
            return_statements=return_count,
            is_async=isinstance(node, ast.AsyncFunctionDef),
            is_recursive=is_recursive,
            docstring_quality=docstring_quality
        )
    
    def _analyze_python_class(self, node: ast.ClassDef, content: str) -> ClassMetrics:
        """Python í´ë˜ìŠ¤ ë¶„ì„"""
        # ë©”ì„œë“œ ë° ì†ì„± ê°œìˆ˜
        methods = [n for n in node.body if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))]
        attributes = [n for n in ast.walk(node) if isinstance(n, ast.Assign)]
        
        # ìƒì† ê¹Šì´
        inheritance_depth = len(node.bases)
        
        # ì¶”ìƒ í´ë˜ìŠ¤ ì—¬ë¶€
        is_abstract = any(isinstance(decorator, ast.Name) and 
                         decorator.id == 'abstractmethod'
                         for method in methods 
                         for decorator in method.decorator_list
                         if isinstance(decorator, ast.Name))
        
        # ë””ìì¸ íŒ¨í„´ íƒì§€
        detected_patterns = self._detect_design_patterns(node, content)
        
        return ClassMetrics(
            name=node.name,
            line_start=node.lineno,
            line_end=getattr(node, 'end_lineno', node.lineno),
            methods_count=len(methods),
            attributes_count=len(attributes),
            inheritance_depth=inheritance_depth,
            is_abstract=is_abstract,
            design_patterns=detected_patterns
        )
    
    def _analyze_python_import(self, node: Union[ast.Import, ast.ImportFrom]) -> List[DependencyInfo]:
        """Python import ë¬¸ ë¶„ì„"""
        dependencies = []
        
        if isinstance(node, ast.Import):
            for alias in node.names:
                module_name = alias.name.split('.')[0]
                dependencies.append(DependencyInfo(
                    name=alias.asname or alias.name,
                    import_type="import",
                    module=alias.name,
                    line=node.lineno,
                    is_external=module_name not in self.python_stdlib,
                    is_standard_library=module_name in self.python_stdlib
                ))
        
        elif isinstance(node, ast.ImportFrom):
            module = node.module or ""
            module_name = module.split('.')[0]
            
            for alias in node.names:
                dependencies.append(DependencyInfo(
                    name=alias.asname or alias.name,
                    import_type="from_import",
                    module=module,
                    line=node.lineno,
                    is_external=module_name not in self.python_stdlib,
                    is_standard_library=module_name in self.python_stdlib
                ))
        
        return dependencies
    
    def _analyze_javascript_file(self, file_path: Path, content: str, result: AnalysisResult) -> AnalysisResult:
        """JavaScript/TypeScript íŒŒì¼ ë¶„ì„"""
        lines = content.splitlines()
        
        # ì •ê·œì‹ ê¸°ë°˜ ë¶„ì„
        for i, line in enumerate(lines, 1):
            # í•¨ìˆ˜ íŒ¨í„´
            func_patterns = [
                r'function\s+(\w+)\s*\(',
                r'const\s+(\w+)\s*=\s*function',
                r'(\w+)\s*:\s*function',
                r'(\w+)\s*=\s*\([^)]*\)\s*=>'
            ]
            
            for pattern in func_patterns:
                match = re.search(pattern, line)
                if match:
                    result.functions.append(FunctionMetrics(
                        name=match.group(1),
                        line_start=i,
                        line_end=i,  # ë‹¨ìˆœí™”
                        parameters_count=line.count(',') + 1 if '(' in line else 0
                    ))
            
            # í´ë˜ìŠ¤ íŒ¨í„´
            class_match = re.search(r'class\s+(\w+)', line)
            if class_match:
                result.classes.append(ClassMetrics(
                    name=class_match.group(1),
                    line_start=i,
                    line_end=i
                ))
            
            # Import íŒ¨í„´
            import_patterns = [
                r'import\s+.*?\s+from\s+[\'"]([^\'"]+)[\'"]',
                r'import\s+[\'"]([^\'"]+)[\'"]',
                r'require\([\'"]([^\'"]+)[\'"]\)'
            ]
            
            for pattern in import_patterns:
                match = re.search(pattern, line)
                if match:
                    result.dependencies.append(DependencyInfo(
                        name=match.group(1),
                        import_type="import",
                        module=match.group(1),
                        line=i,
                        is_external=not match.group(1).startswith('.')
                    ))
        
        return result
    
    def _analyze_generic_file(self, file_path: Path, content: str, result: AnalysisResult) -> AnalysisResult:
        """ì¼ë°˜ íŒŒì¼ ë¶„ì„"""
        return self._analyze_with_regex(content, result)
    
    def _analyze_with_regex(self, content: str, result: AnalysisResult) -> AnalysisResult:
        """ì •ê·œì‹ ê¸°ë°˜ í´ë°± ë¶„ì„"""
        lines = content.splitlines()
        
        for i, line in enumerate(lines, 1):
            # í•¨ìˆ˜ íŒ¨í„´
            if re.search(r'\w+\s*\([^)]*\)\s*\{', line):
                func_match = re.search(r'(\w+)\s*\(', line)
                if func_match:
                    result.functions.append(FunctionMetrics(
                        name=func_match.group(1),
                        line_start=i,
                        line_end=i
                    ))
        
        return result
    
    def _analyze_security(self, content: str) -> List[SecurityIssue]:
        """ë³´ì•ˆ ë¶„ì„"""
        issues = []
        lines = content.splitlines()
        
        for category, patterns in self.security_patterns.items():
            for pattern, severity, description in patterns:
                for i, line in enumerate(lines, 1):
                    if re.search(pattern, line):
                        issues.append(SecurityIssue(
                            type=category,
                            severity=severity,
                            line=i,
                            description=description,
                            recommendation=self._get_security_recommendation(category),
                            code_snippet=line.strip()
                        ))
        
        return issues
    
    def _extract_env_vars(self, content: str, language: LanguageType) -> List[Dict[str, Any]]:
        """í™˜ê²½ë³€ìˆ˜ ì¶”ì¶œ"""
        env_vars = []
        lines = content.splitlines()
        
        if language == LanguageType.PYTHON:
            patterns = [
                r'os\.environ\[[\'"](.*?)[\'"]\]',
                r'os\.getenv\([\'"](.*?)[\'"]\)',
            ]
        else:
            patterns = [
                r'process\.env\.([A-Z_][A-Z0-9_]*)',
                r'process\.env\[[\'"](.*?)[\'"]\]',
            ]
        
        for i, line in enumerate(lines, 1):
            for pattern in patterns:
                matches = re.finditer(pattern, line)
                for match in matches:
                    env_vars.append({
                        'name': match.group(1),
                        'line': i,
                        'pattern': pattern,
                        'context': line.strip()
                    })
        
        return env_vars
    
    def _calculate_quality_metrics(self, result: AnalysisResult) -> CodeQualityMetrics:
        """ì½”ë“œ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        # ë³µì¡ë„ í‰ê· 
        avg_complexity = 0
        if result.functions:
            avg_complexity = sum(f.cyclomatic_complexity for f in result.functions) / len(result.functions)
        
        # ìœ ì§€ë³´ìˆ˜ì„± ì§€ìˆ˜ (Microsoft ê³µì‹ ê¸°ë°˜)
        maintainability = max(0, 171 - 5.2 * avg_complexity - 0.23 * len(result.functions) - 16.2 * len(str(result.lines_of_code)))
        
        # ë¬¸ì„œí™” ë¹„ìœ¨
        documented_functions = sum(1 for f in result.functions if f.docstring_quality > 0)
        doc_ratio = documented_functions / len(result.functions) if result.functions else 0
        
        # ë³µì¡ë„ ë¶„í¬
        complexity_dist = {}
        for func in result.functions:
            complexity = func.cyclomatic_complexity
            if complexity <= 5:
                complexity_dist['low'] = complexity_dist.get('low', 0) + 1
            elif complexity <= 10:
                complexity_dist['medium'] = complexity_dist.get('medium', 0) + 1
            else:
                complexity_dist['high'] = complexity_dist.get('high', 0) + 1
        
        return CodeQualityMetrics(
            maintainability_index=maintainability,
            documentation_ratio=doc_ratio,
            complexity_distribution=complexity_dist
        )
    
    def _calculate_cyclomatic_complexity(self, node: ast.AST) -> int:
        """ìˆœí™˜ ë³µì¡ë„ ê³„ì‚°"""
        complexity = 1
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, (ast.And, ast.Or)):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
        return complexity
    
    def _calculate_cognitive_complexity(self, node: ast.AST) -> int:
        """ì¸ì§€ ë³µì¡ë„ ê³„ì‚° (SonarQube ë°©ì‹)"""
        complexity = 0
        nesting_level = 0
        
        def calculate_recursive(node, level=0):
            nonlocal complexity
            
            for child in ast.iter_child_nodes(node):
                if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                    complexity += 1 + level
                    calculate_recursive(child, level + 1)
                elif isinstance(child, ast.ExceptHandler):
                    complexity += 1 + level
                    calculate_recursive(child, level + 1)
                elif isinstance(child, (ast.And, ast.Or)):
                    complexity += 1
                    calculate_recursive(child, level)
                else:
                    calculate_recursive(child, level)
        
        calculate_recursive(node)
        return complexity
    
    def _calculate_halstead_complexity(self, node: ast.AST, content: str) -> Dict[str, float]:
        """í• ìŠ¤í…Œë“œ ë³µì¡ë„ ê³„ì‚°"""
        operators = set()
        operands = set()
        operator_count = 0
        operand_count = 0
        
        for child in ast.walk(node):
            if isinstance(child, ast.BinOp):
                operators.add(type(child.op).__name__)
                operator_count += 1
            elif isinstance(child, ast.Name):
                operands.add(child.id)
                operand_count += 1
            elif isinstance(child, ast.Constant):
                operands.add(str(child.value))
                operand_count += 1
        
        # í• ìŠ¤í…Œë“œ ë©”íŠ¸ë¦­ ê³„ì‚°
        n1 = len(operators)  # ê³ ìœ  ì—°ì‚°ì ìˆ˜
        n2 = len(operands)   # ê³ ìœ  í”¼ì—°ì‚°ì ìˆ˜
        N1 = operator_count  # ì´ ì—°ì‚°ì ìˆ˜
        N2 = operand_count   # ì´ í”¼ì—°ì‚°ì ìˆ˜
        
        if n1 == 0 or n2 == 0:
            return {}
        
        vocabulary = n1 + n2
        length = N1 + N2
        volume = length * (vocabulary.bit_length() if vocabulary > 0 else 0)
        difficulty = (n1 / 2) * (N2 / n2) if n2 > 0 else 0
        effort = difficulty * volume
        
        return {
            'vocabulary': vocabulary,
            'length': length,
            'volume': volume,
            'difficulty': difficulty,
            'effort': effort
        }
    
    def _evaluate_docstring_quality(self, docstring: Optional[str]) -> int:
        """ë…ìŠ¤íŠ¸ë§ í’ˆì§ˆ í‰ê°€ (0-100ì )"""
        if not docstring:
            return 0
        
        score = 0
        
        # ê¸°ë³¸ ì¡´ì¬ ì ìˆ˜
        score += 20
        
        # ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ì§€ ì•Šê³  ë„ˆë¬´ ê¸¸ì§€ ì•ŠìŒ)
        if 10 <= len(docstring) <= 500:
            score += 20
        elif len(docstring) > 10:
            score += 10
        
        # êµ¬ì¡° ì ìˆ˜
        if 'Args:' in docstring or 'Parameters:' in docstring:
            score += 20
        if 'Returns:' in docstring or 'Return:' in docstring:
            score += 20
        if 'Raises:' in docstring or 'Throws:' in docstring:
            score += 10
        if 'Example:' in docstring or 'Examples:' in docstring:
            score += 10
        
        return min(score, 100)
    
    def _detect_design_patterns(self, node: ast.ClassDef, content: str) -> List[str]:
        """ë””ìì¸ íŒ¨í„´ íƒì§€"""
        patterns = []
        class_content = ast.get_source_segment(content, node)
        
        if not class_content:
            return patterns
        
        for pattern_name, pattern_regexes in self.design_patterns.items():
            for regex in pattern_regexes:
                if re.search(regex, class_content, re.MULTILINE):
                    patterns.append(pattern_name)
                    break
        
        return patterns
    
    def _get_security_recommendation(self, issue_type: str) -> str:
        """ë³´ì•ˆ ì´ìŠˆë³„ ê¶Œì¥ì‚¬í•­"""
        recommendations = {
            'hardcoded_secrets': "í™˜ê²½ë³€ìˆ˜ë‚˜ ì„¤ì • íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ë¯¼ê°í•œ ì •ë³´ë¥¼ ì™¸ë¶€í™”í•˜ì„¸ìš”.",
            'sql_injection': "ë§¤ê°œë³€ìˆ˜í™”ëœ ì¿¼ë¦¬ë‚˜ ORMì„ ì‚¬ìš©í•˜ì„¸ìš”.",
            'xss_vulnerability': "ì‚¬ìš©ì ì…ë ¥ì„ ì ì ˆíˆ ì´ìŠ¤ì¼€ì´í”„í•˜ê±°ë‚˜ ê²€ì¦í•˜ì„¸ìš”.",
            'weak_crypto': "SHA-256 ì´ìƒì˜ ê°•ë ¥í•œ í•´ì‹œ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì„¸ìš”."
        }
        return recommendations.get(issue_type, "ë³´ì•ˆ ëª¨ë²” ì‚¬ë¡€ë¥¼ ë”°ë¥´ì„¸ìš”.")
    
    def _generate_project_summary(self, analysis_results: List[AnalysisResult]) -> ProjectSummary:
        """í”„ë¡œì íŠ¸ ìš”ì•½ ìƒì„±"""
        if not analysis_results:
            return ProjectSummary()
        
        # ê¸°ë³¸ í†µê³„
        total_files = len(analysis_results)
        total_lines = sum(r.lines_of_code for r in analysis_results)
        
        # ì–¸ì–´ë³„ ë¶„í¬
        language_dist = {}
        for result in analysis_results:
            lang = result.language.value
            language_dist[lang] = language_dist.get(lang, 0) + 1
        
        # ë³µì¡ë„ ìš”ì•½
        all_functions = [f for r in analysis_results for f in r.functions]
        complexity_summary = {}
        if all_functions:
            complexities = [f.cyclomatic_complexity for f in all_functions]
            complexity_summary = {
                'avg_cyclomatic': sum(complexities) / len(complexities),
                'max_cyclomatic': max(complexities),
                'functions_over_10': len([c for c in complexities if c > 10])
            }
        
        # ë³´ì•ˆ ìš”ì•½
        all_security_issues = [issue for r in analysis_results for issue in r.security_issues]
        security_summary = {}
        for issue in all_security_issues:
            severity = issue.severity.value
            security_summary[severity] = security_summary.get(severity, 0) + 1
        
        # í’ˆì§ˆ ìš”ì•½
        quality_metrics = [r.quality_metrics for r in analysis_results]
        quality_summary = {}
        if quality_metrics:
            maintainability_scores = [q.maintainability_index for q in quality_metrics]
            doc_ratios = [q.documentation_ratio for q in quality_metrics]
            
            quality_summary = {
                'avg_maintainability': sum(maintainability_scores) / len(maintainability_scores),
                'avg_documentation': sum(doc_ratios) / len(doc_ratios)
            }
        
        # ì˜ì¡´ì„± ê·¸ë˜í”„
        dependency_graph = self.build_dependency_graph(analysis_results)
        
        # í•«ìŠ¤íŒŸ ë¶„ì„ (ë³µì¡ë„ê°€ ë†’ì€ íŒŒì¼ë“¤)
        hotspots = []
        for result in analysis_results:
            if result.functions:
                avg_complexity = sum(f.cyclomatic_complexity for f in result.functions) / len(result.functions)
                if avg_complexity > 10:
                    hotspots.append({
                        'file': result.file_path,
                        'avg_complexity': avg_complexity,
                        'functions_count': len(result.functions),
                        'security_issues': len(result.security_issues)
                    })
        
        hotspots.sort(key=lambda x: x['avg_complexity'], reverse=True)
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(analysis_results, complexity_summary, security_summary)
        
        return ProjectSummary(
            total_files=total_files,
            total_lines=total_lines,
            language_distribution=language_dist,
            complexity_summary=complexity_summary,
            security_summary=security_summary,
            quality_summary=quality_summary,
            dependency_graph=dependency_graph,
            hotspots=hotspots[:10],  # ìƒìœ„ 10ê°œë§Œ
            recommendations=recommendations
        )
    
    def _generate_recommendations(self, analysis_results: List[AnalysisResult], 
                                complexity_summary: Dict, security_summary: Dict) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ë³µì¡ë„ ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        if complexity_summary.get('functions_over_10', 0) > 0:
            recommendations.append(
                f"ìˆœí™˜ ë³µì¡ë„ê°€ 10ì„ ì´ˆê³¼í•˜ëŠ” í•¨ìˆ˜ê°€ {complexity_summary['functions_over_10']}ê°œ ìˆìŠµë‹ˆë‹¤. "
                "í•¨ìˆ˜ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”."
            )
        
        # ë³´ì•ˆ ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        critical_issues = security_summary.get('critical', 0)
        if critical_issues > 0:
            recommendations.append(
                f"ì‹¬ê°í•œ ë³´ì•ˆ ì´ìŠˆê°€ {critical_issues}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. "
                "í•˜ë“œì½”ë”©ëœ ì‹œí¬ë¦¿ì„ í™˜ê²½ë³€ìˆ˜ë¡œ ì´ë™í•˜ì„¸ìš”."
            )
        
        # ë¬¸ì„œí™” ê¶Œì¥ì‚¬í•­
        all_functions = [f for r in analysis_results for f in r.functions]
        undocumented = len([f for f in all_functions if f.docstring_quality == 0])
        if undocumented > len(all_functions) * 0.5:
            recommendations.append(
                f"ë¬¸ì„œí™”ë˜ì§€ ì•Šì€ í•¨ìˆ˜ê°€ {undocumented}ê°œ ìˆìŠµë‹ˆë‹¤. "
                "ì½”ë“œ ê°€ë…ì„±ì„ ìœ„í•´ ë…ìŠ¤íŠ¸ë§ì„ ì¶”ê°€í•˜ì„¸ìš”."
            )
        
        # ì˜ì¡´ì„± ê¶Œì¥ì‚¬í•­
        external_deps = set()
        for result in analysis_results:
            for dep in result.dependencies:
                if dep.is_external:
                    external_deps.add(dep.module)
        
        if len(external_deps) > 20:
            recommendations.append(
                f"ì™¸ë¶€ ì˜ì¡´ì„±ì´ {len(external_deps)}ê°œë¡œ ë§ìŠµë‹ˆë‹¤. "
                "ì˜ì¡´ì„±ì„ ì •ë¦¬í•˜ê³  í•„ìš”í•œ ê²ƒë§Œ ìœ ì§€í•˜ì„¸ìš”."
            )
        
        return recommendations
    
    def _detect_language(self, file_path: Path) -> LanguageType:
        """íŒŒì¼ í™•ì¥ìë¡œ ì–¸ì–´ íƒì§€"""
        suffix = file_path.suffix.lower()
        return self.language_mapping.get(suffix, LanguageType.UNKNOWN)
    
    def _get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return ""
    
    def _should_skip_file(self, file_path: Path) -> bool:
        """íŒŒì¼ì„ ê±´ë„ˆë›¸ì§€ íŒë‹¨"""
        skip_patterns = [
            'node_modules', '.git', '__pycache__', 'venv', '.venv',
            'dist', 'build', '.next', '.nuxt', 'target', 'vendor',
            '.pytest_cache', '.mypy_cache', 'coverage'
        ]
        
        path_str = str(file_path)
        return any(pattern in path_str for pattern in skip_patterns)


class SecurityAnalyzer:
    """ì „ë¬¸ ë³´ì•ˆ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.vulnerability_patterns = {
            'injection': {
                'sql': [
                    r'(?i)(select|insert|update|delete).*(\+|format|%)',
                    r'(?i)cursor\.execute\s*\(\s*["\'].*\+',
                    r'(?i)query\s*=\s*["\'].*\+.*["\']'
                ],
                'command': [
                    r'(?i)(os\.system|subprocess\.call|exec|eval)\s*\([^)]*\+',
                    r'(?i)shell=True.*\+',
                ],
                'ldap': [
                    r'(?i)ldap.*search.*\+',
                ]
            },
            'crypto': {
                'weak_algorithms': [
                    r'(?i)(md5|sha1|des|rc4)\s*\(',
                    r'(?i)algorithm\s*=\s*["\'](?:md5|sha1|des)["\']'
                ],
                'weak_random': [
                    r'(?i)random\.random\(\)',
                    r'(?i)math\.random\(\)'
                ]
            },
            'authentication': {
                'weak_password': [
                    r'(?i)password\s*=\s*["\']["\']',  # ë¹ˆ íŒ¨ìŠ¤ì›Œë“œ
                    r'(?i)password\s*=\s*["\'](?:admin|password|123|test)["\']'
                ],
                'hardcoded_credentials': [
                    r'(?i)(api_key|secret|token|password)\s*=\s*["\'][^"\']{8,}["\']'
                ]
            }
        }
    
    def analyze_vulnerabilities(self, content: str, language: LanguageType) -> List[SecurityIssue]:
        """ì‹¬í™” ë³´ì•ˆ ì·¨ì•½ì  ë¶„ì„"""
        issues = []
        lines = content.splitlines()
        
        for category, subcategories in self.vulnerability_patterns.items():
            for subcategory, patterns in subcategories.items():
                for pattern in patterns:
                    for i, line in enumerate(lines, 1):
                        if re.search(pattern, line):
                            severity = self._determine_severity(category, subcategory)
                            issues.append(SecurityIssue(
                                type=f"{category}_{subcategory}",
                                severity=severity,
                                line=i,
                                description=self._get_vulnerability_description(category, subcategory),
                                recommendation=self._get_vulnerability_recommendation(category, subcategory),
                                code_snippet=line.strip()
                            ))
        
        return issues
    
    def _determine_severity(self, category: str, subcategory: str) -> SecurityLevel:
        """ì·¨ì•½ì  ì‹¬ê°ë„ ê²°ì •"""
        severity_mapping = {
            ('injection', 'sql'): SecurityLevel.CRITICAL,
            ('injection', 'command'): SecurityLevel.CRITICAL,
            ('crypto', 'weak_algorithms'): SecurityLevel.HIGH,
            ('authentication', 'hardcoded_credentials'): SecurityLevel.CRITICAL,
            ('authentication', 'weak_password'): SecurityLevel.HIGH,
        }
        return severity_mapping.get((category, subcategory), SecurityLevel.MEDIUM)
    
    def _get_vulnerability_description(self, category: str, subcategory: str) -> str:
        """ì·¨ì•½ì  ì„¤ëª…"""
        descriptions = {
            ('injection', 'sql'): "SQL ì¸ì ì…˜ ì·¨ì•½ì ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤",
            ('injection', 'command'): "ëª…ë ¹ì–´ ì¸ì ì…˜ ì·¨ì•½ì ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤",
            ('crypto', 'weak_algorithms'): "ì•½í•œ ì•”í˜¸í™” ì•Œê³ ë¦¬ì¦˜ì´ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤",
            ('authentication', 'hardcoded_credentials'): "í•˜ë“œì½”ë”©ëœ ì¸ì¦ ì •ë³´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤",
        }
        return descriptions.get((category, subcategory), "ë³´ì•ˆ ì·¨ì•½ì ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    def _get_vulnerability_recommendation(self, category: str, subcategory: str) -> str:
        """ì·¨ì•½ì ë³„ ê¶Œì¥ì‚¬í•­"""
        recommendations = {
            ('injection', 'sql'): "ë§¤ê°œë³€ìˆ˜í™”ëœ ì¿¼ë¦¬ë‚˜ ORMì„ ì‚¬ìš©í•˜ì„¸ìš”",
            ('injection', 'command'): "ì…ë ¥ê°’ì„ ê²€ì¦í•˜ê³  í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë°©ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”",
            ('crypto', 'weak_algorithms'): "SHA-256, AES ë“± ê°•ë ¥í•œ ì•”í˜¸í™” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì„¸ìš”",
            ('authentication', 'hardcoded_credentials'): "í™˜ê²½ë³€ìˆ˜ë‚˜ í‚¤ ê´€ë¦¬ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”",
        }
        return recommendations.get((category, subcategory), "ë³´ì•ˆ ëª¨ë²” ì‚¬ë¡€ë¥¼ ë”°ë¥´ì„¸ìš”")


# í¸ì˜ í•¨ìˆ˜ë“¤
def analyze_single_file(file_path: Union[str, Path], 
                       cache_enabled: bool = True) -> AnalysisResult:
    """ë‹¨ì¼ íŒŒì¼ ë¶„ì„ í¸ì˜ í•¨ìˆ˜"""
    analyzer = EnhancedProjectAnalyzer(cache_enabled=cache_enabled)
    return analyzer.analyze_file(file_path)


def analyze_project_enhanced(project_path: Union[str, Path], 
                           recursive: bool = True,
                           max_workers: int = 4) -> Tuple[List[AnalysisResult], ProjectSummary]:
    """í”„ë¡œì íŠ¸ ë¶„ì„ í¸ì˜ í•¨ìˆ˜"""
    analyzer = EnhancedProjectAnalyzer(max_workers=max_workers)
    return analyzer.analyze_project(project_path, recursive)


def get_security_report(analysis_results: List[AnalysisResult]) -> Dict[str, Any]:
    """ë³´ì•ˆ ë¦¬í¬íŠ¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    all_issues = []
    for result in analysis_results:
        for issue in result.security_issues:
            all_issues.append({
                'file': result.file_path,
                'type': issue.type,
                'severity': issue.severity.value,
                'line': issue.line,
                'description': issue.description,
                'code': issue.code_snippet
            })
    
    # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
    by_severity = {}
    for issue in all_issues:
        severity = issue['severity']
        if severity not in by_severity:
            by_severity[severity] = []
        by_severity[severity].append(issue)
    
    return {
        'total_issues': len(all_issues),
        'by_severity': by_severity,
        'top_files_with_issues': _get_top_vulnerable_files(all_issues),
        'recommendations': _generate_security_recommendations(by_severity)
    }


def get_complexity_report(analysis_results: List[AnalysisResult]) -> Dict[str, Any]:
    """ë³µì¡ë„ ë¦¬í¬íŠ¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    all_functions = []
    for result in analysis_results:
        for func in result.functions:
            all_functions.append({
                'file': result.file_path,
                'name': func.name,
                'cyclomatic': func.cyclomatic_complexity,
                'cognitive': func.cognitive_complexity,
                'lines': func.line_end - func.line_start + 1
            })
    
    # ë³µì¡ë„ í†µê³„
    if all_functions:
        complexities = [f['cyclomatic'] for f in all_functions]
        complex_functions = [f for f in all_functions if f['cyclomatic'] > 10]
        
        return {
            'total_functions': len(all_functions),
            'avg_complexity': sum(complexities) / len(complexities),
            'max_complexity': max(complexities),
            'complex_functions_count': len(complex_functions),
            'complex_functions': sorted(complex_functions, 
                                      key=lambda x: x['cyclomatic'], 
                                      reverse=True)[:10]
        }
    
    return {'total_functions': 0}


def _get_top_vulnerable_files(issues: List[Dict]) -> List[Dict]:
    """ì·¨ì•½ì ì´ ë§ì€ íŒŒì¼ ìˆœìœ„"""
    file_counts = {}
    for issue in issues:
        file_path = issue['file']
        if file_path not in file_counts:
            file_counts[file_path] = {'count': 0, 'critical': 0, 'high': 0}
        file_counts[file_path]['count'] += 1
        if issue['severity'] == 'critical':
            file_counts[file_path]['critical'] += 1
        elif issue['severity'] == 'high':
            file_counts[file_path]['high'] += 1
    
    sorted_files = sorted(file_counts.items(), 
                         key=lambda x: (x[1]['critical'], x[1]['high'], x[1]['count']), 
                         reverse=True)
    
    return [{'file': file, **stats} for file, stats in sorted_files[:5]]


def _generate_security_recommendations(by_severity: Dict) -> List[str]:
    """ë³´ì•ˆ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
    recommendations = []
    
    if 'critical' in by_severity:
        recommendations.append(
            f"ê¸´ê¸‰: {len(by_severity['critical'])}ê°œì˜ ì‹¬ê°í•œ ë³´ì•ˆ ì·¨ì•½ì ì„ ì¦‰ì‹œ ìˆ˜ì •í•˜ì„¸ìš”."
        )
    
    if 'high' in by_severity:
        recommendations.append(
            f"ë†’ìŒ: {len(by_severity['high'])}ê°œì˜ ë†’ì€ ìœ„í—˜ ì·¨ì•½ì ì„ ìš°ì„  ìˆ˜ì •í•˜ì„¸ìš”."
        )
    
    recommendations.extend([
        "ì •ê¸°ì ì¸ ë³´ì•ˆ ì½”ë“œ ë¦¬ë·°ë¥¼ ì‹¤ì‹œí•˜ì„¸ìš”.",
        "ìë™í™”ëœ ë³´ì•ˆ ìŠ¤ìº” ë„êµ¬ë¥¼ CI/CD íŒŒì´í”„ë¼ì¸ì— í†µí•©í•˜ì„¸ìš”.",
        "ê°œë°œíŒ€ì— ë³´ì•ˆ ì½”ë”© êµìœ¡ì„ ì œê³µí•˜ì„¸ìš”."
    ])
    
    return recommendations


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    import sys
    
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python analysis_utils.py <í”„ë¡œì íŠ¸_ê²½ë¡œ>")
        sys.exit(1)
    
    project_path = sys.argv[1]
    
    # í”„ë¡œì íŠ¸ ë¶„ì„ ì‹¤í–‰
    results, summary = analyze_project_enhanced(project_path)
    
    print(f"ë¶„ì„ ì™„ë£Œ: {summary.total_files}ê°œ íŒŒì¼, {summary.total_lines}ì¤„")
    print(f"ë³´ì•ˆ ì´ìŠˆ: {sum(summary.security_summary.values())}ê°œ")
    print(f"ë³µì¡í•œ í•¨ìˆ˜: {summary.complexity_summary.get('functions_over_10', 0)}ê°œ")
    
    # ë³´ì•ˆ ë¦¬í¬íŠ¸
    security_report = get_security_report(results)
    if security_report['total_issues'] > 0:
        print(f"\nâš ï¸  ë³´ì•ˆ ì´ìŠˆ {security_report['total_issues']}ê°œ ë°œê²¬")
        for severity, issues in security_report['by_severity'].items():
            print(f"  {severity}: {len(issues)}ê°œ")
    
    # ë³µì¡ë„ ë¦¬í¬íŠ¸
    complexity_report = get_complexity_report(results)
    if complexity_report['complex_functions_count'] > 0:
        print(f"\nğŸ“Š ë³µì¡í•œ í•¨ìˆ˜ {complexity_report['complex_functions_count']}ê°œ")
        print(f"  í‰ê·  ë³µì¡ë„: {complexity_report['avg_complexity']:.1f}")
        print(f"  ìµœëŒ€ ë³µì¡ë„: {complexity_report['max_complexity']}")
