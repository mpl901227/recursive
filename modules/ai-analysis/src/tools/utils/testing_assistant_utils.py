#!/usr/bin/env python3
"""
Testing Assistant Utilities
AI ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±, ì»¤ë²„ë¦¬ì§€ ë¶„ì„, í’ˆì§ˆ ê²€ì¦ ë„êµ¬ë“¤

ì£¼ìš” ê¸°ëŠ¥:
- ìë™ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± (ë‹¨ìœ„, í†µí•©, E2E)
- í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ë° ì‹œê°í™”
- ì—£ì§€ ì¼€ì´ìŠ¤ ìë™ íƒì§€
- í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
- ëª¨í‚¹ ë° ìŠ¤í… ìë™ ìƒì„±
- ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì§€ì›
- í…ŒìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€
- ë¦¬ê·¸ë ˆì…˜ í…ŒìŠ¤íŠ¸ ê´€ë¦¬
"""

import ast
import inspect
import json
import random
import re
import subprocess
import sys
import time
import uuid
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import (
    Any, Dict, List, Optional, Set, Tuple, Union, Callable, 
    Type, Iterator, Generator, Protocol, runtime_checkable
)
import logging
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
import pickle
from contextlib import contextmanager
import traceback
import importlib.util
import tempfile
import shutil

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


# ì—´ê±°í˜• ì •ì˜
class TestType(Enum):
    UNIT = "unit"
    INTEGRATION = "integration"
    E2E = "e2e"
    PERFORMANCE = "performance"
    SECURITY = "security"
    REGRESSION = "regression"


class TestFramework(Enum):
    PYTEST = "pytest"
    UNITTEST = "unittest"
    NOSE = "nose"
    JEST = "jest"
    MOCHA = "mocha"


class CoverageType(Enum):
    LINE = "line"
    BRANCH = "branch"
    FUNCTION = "function"
    STATEMENT = "statement"


class TestPriority(Enum):
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


# ë°ì´í„° í´ë˜ìŠ¤ë“¤
@dataclass
class TestCase:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ë³´"""
    name: str
    test_type: TestType
    function_name: str
    test_code: str
    description: str
    priority: TestPriority = TestPriority.MEDIUM
    tags: List[str] = field(default_factory=list)
    dependencies: List[str] = field(default_factory=list)
    expected_runtime: float = 0.0
    setup_code: Optional[str] = None
    teardown_code: Optional[str] = None
    mock_requirements: List[str] = field(default_factory=list)
    data_requirements: Dict[str, Any] = field(default_factory=dict)
    edge_cases: List[str] = field(default_factory=list)


@dataclass
class CoverageReport:
    """ì»¤ë²„ë¦¬ì§€ ë³´ê³ ì„œ"""
    total_lines: int
    covered_lines: int
    line_coverage: float
    branch_coverage: float
    function_coverage: float
    missing_lines: List[int]
    uncovered_branches: List[Tuple[int, int]]
    uncovered_functions: List[str]
    file_coverage: Dict[str, float]
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê²°ê³¼"""
    test_name: str
    status: str  # passed, failed, skipped, error
    execution_time: float
    error_message: Optional[str] = None
    traceback: Optional[str] = None
    assertions_count: int = 0
    warnings: List[str] = field(default_factory=list)
    memory_usage: float = 0.0
    coverage_impact: float = 0.0


@dataclass
class TestSuite:
    """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    name: str
    test_cases: List[TestCase]
    setup_code: Optional[str] = None
    teardown_code: Optional[str] = None
    parallel_execution: bool = False
    timeout: Optional[float] = None
    retry_count: int = 0


@dataclass
class EdgeCase:
    """ì—£ì§€ ì¼€ì´ìŠ¤ ì •ë³´"""
    name: str
    description: str
    input_values: Dict[str, Any]
    expected_behavior: str
    category: str  # boundary, null, overflow, etc.
    severity: TestPriority = TestPriority.MEDIUM


@dataclass
class MockSpec:
    """ëª¨í‚¹ ì‚¬ì–‘"""
    target: str
    mock_type: str  # function, class, module
    return_value: Any = None
    side_effect: Optional[Callable] = None
    spec: Optional[Type] = None
    attributes: Dict[str, Any] = field(default_factory=dict)


# í”„ë¡œí† ì½œ ì •ì˜
@runtime_checkable
class TestGenerator(Protocol):
    def generate_tests(self, function_code: str) -> List[TestCase]:
        """í…ŒìŠ¤íŠ¸ ìƒì„±"""
        ...


@runtime_checkable
class CoverageAnalyzer(Protocol):
    def analyze_coverage(self, source_files: List[str], test_files: List[str]) -> CoverageReport:
        """ì»¤ë²„ë¦¬ì§€ ë¶„ì„"""
        ...


# ì»¤ìŠ¤í…€ ì˜ˆì™¸
class TestGenerationError(Exception):
    """í…ŒìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜"""
    pass


class CoverageAnalysisError(Exception):
    """ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì˜¤ë¥˜"""
    pass


class TestExecutionError(Exception):
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜"""
    pass


class FunctionAnalyzer:
    """í•¨ìˆ˜ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def analyze_function(self, function_code: str) -> Dict[str, Any]:
        """í•¨ìˆ˜ ë¶„ì„"""
        try:
            tree = ast.parse(function_code)
            
            analysis = {
                "name": "",
                "parameters": [],
                "return_type": None,
                "complexity": 1,
                "dependencies": [],
                "exceptions": [],
                "side_effects": [],
                "pure_function": True,
                "async_function": False,
                "decorators": [],
                "docstring": None
            }
            
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    analysis["name"] = node.name
                    analysis["async_function"] = isinstance(node, ast.AsyncFunctionDef)
                    analysis["docstring"] = ast.get_docstring(node)
                    
                    # ë§¤ê°œë³€ìˆ˜ ë¶„ì„
                    for arg in node.args.args:
                        param_info = {
                            "name": arg.arg,
                            "annotation": self._get_annotation(arg.annotation) if arg.annotation else None,
                            "default": None
                        }
                        analysis["parameters"].append(param_info)
                    
                    # ê¸°ë³¸ê°’ ì²˜ë¦¬
                    defaults = node.args.defaults
                    if defaults:
                        param_count = len(analysis["parameters"])
                        default_count = len(defaults)
                        for i, default in enumerate(defaults):
                            param_index = param_count - default_count + i
                            if param_index >= 0:
                                analysis["parameters"][param_index]["default"] = self._get_literal_value(default)
                    
                    # ë°˜í™˜ íƒ€ì…
                    if node.returns:
                        analysis["return_type"] = self._get_annotation(node.returns)
                    
                    # ë°ì½”ë ˆì´í„°
                    for decorator in node.decorator_list:
                        analysis["decorators"].append(self._get_decorator_name(decorator))
                
                # ë³µì¡ë„ ê³„ì‚°
                elif isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                    analysis["complexity"] += 1
                elif isinstance(node, ast.ExceptHandler):
                    analysis["complexity"] += 1
                
                # ì˜ˆì™¸ ì²˜ë¦¬
                elif isinstance(node, ast.Raise):
                    if node.exc:
                        exc_name = self._get_exception_name(node.exc)
                        if exc_name:
                            analysis["exceptions"].append(exc_name)
                
                # ì™¸ë¶€ í˜¸ì¶œ (ìˆœìˆ˜ì„± ê²€ì‚¬)
                elif isinstance(node, ast.Call):
                    func_name = self._get_call_name(node)
                    if func_name:
                        analysis["dependencies"].append(func_name)
                        # íŒŒì¼ I/O, ë„¤íŠ¸ì›Œí¬, ì „ì—­ ë³€ìˆ˜ ìˆ˜ì • ë“± ë¶€ì‘ìš© ê²€ì‚¬
                        if self._is_side_effect_call(func_name):
                            analysis["side_effects"].append(func_name)
                            analysis["pure_function"] = False
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"í•¨ìˆ˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _get_annotation(self, node: ast.AST) -> str:
        """íƒ€ì… ì–´ë…¸í…Œì´ì…˜ ì¶”ì¶œ"""
        try:
            if hasattr(ast, 'unparse'):
                return ast.unparse(node)
            else:
                return str(node)
        except Exception:
            return "Any"
    
    def _get_literal_value(self, node: ast.AST) -> Any:
        """ë¦¬í„°ëŸ´ ê°’ ì¶”ì¶œ"""
        try:
            if isinstance(node, ast.Constant):
                return node.value
            elif isinstance(node, ast.Num):  # Python < 3.8
                return node.n
            elif isinstance(node, ast.Str):  # Python < 3.8
                return node.s
            elif isinstance(node, ast.NameConstant):  # Python < 3.8
                return node.value
            elif isinstance(node, ast.Name):
                return node.id
            else:
                return None
        except Exception:
            return None
    
    def _get_decorator_name(self, node: ast.AST) -> str:
        """ë°ì½”ë ˆì´í„° ì´ë¦„ ì¶”ì¶œ"""
        try:
            if isinstance(node, ast.Name):
                return node.id
            elif isinstance(node, ast.Attribute):
                return f"{self._get_decorator_name(node.value)}.{node.attr}"
            else:
                return str(node)
        except Exception:
            return "unknown"
    
    def _get_exception_name(self, node: ast.AST) -> Optional[str]:
        """ì˜ˆì™¸ ì´ë¦„ ì¶”ì¶œ"""
        try:
            if isinstance(node, ast.Name):
                return node.id
            elif isinstance(node, ast.Attribute):
                return f"{self._get_exception_name(node.value)}.{node.attr}"
            elif isinstance(node, ast.Call):
                return self._get_exception_name(node.func)
            else:
                return None
        except Exception:
            return None
    
    def _get_call_name(self, node: ast.Call) -> Optional[str]:
        """í•¨ìˆ˜ í˜¸ì¶œ ì´ë¦„ ì¶”ì¶œ"""
        try:
            if isinstance(node.func, ast.Name):
                return node.func.id
            elif isinstance(node.func, ast.Attribute):
                return f"{self._get_call_name_from_attr(node.func.value)}.{node.func.attr}"
            else:
                return None
        except Exception:
            return None
    
    def _get_call_name_from_attr(self, node: ast.AST) -> str:
        """ì†ì„±ì—ì„œ í˜¸ì¶œ ì´ë¦„ ì¶”ì¶œ"""
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            return f"{self._get_call_name_from_attr(node.value)}.{node.attr}"
        else:
            return "unknown"
    
    def _is_side_effect_call(self, func_name: str) -> bool:
        """ë¶€ì‘ìš©ì´ ìˆëŠ” í˜¸ì¶œì¸ì§€ í™•ì¸"""
        side_effect_patterns = [
            "print", "open", "write", "read", "input", "requests.",
            "urllib.", "socket.", "os.", "sys.", "subprocess.",
            "time.sleep", "random.seed", "global", "nonlocal"
        ]
        
        return any(pattern in func_name for pattern in side_effect_patterns)


class TestDataGenerator:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ê¸°"""
    
    def __init__(self):
        self.random = random.Random()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def generate_test_data(self, param_type: str, constraints: Optional[Dict] = None) -> Any:
        """íƒ€ì…ì— ë”°ë¥¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        constraints = constraints or {}
        
        try:
            if param_type in ["int", "integer"]:
                return self._generate_int_data(constraints)
            elif param_type in ["float", "double"]:
                return self._generate_float_data(constraints)
            elif param_type in ["str", "string"]:
                return self._generate_string_data(constraints)
            elif param_type in ["bool", "boolean"]:
                return self._generate_bool_data()
            elif param_type in ["list", "List"]:
                return self._generate_list_data(constraints)
            elif param_type in ["dict", "Dict"]:
                return self._generate_dict_data(constraints)
            elif param_type in ["tuple", "Tuple"]:
                return self._generate_tuple_data(constraints)
            elif param_type in ["set", "Set"]:
                return self._generate_set_data(constraints)
            else:
                return self._generate_custom_data(param_type, constraints)
        except Exception as e:
            self.logger.error(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_int_data(self, constraints: Dict) -> List[int]:
        """ì •ìˆ˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        min_val = constraints.get("min", -1000)
        max_val = constraints.get("max", 1000)
        
        data = [
            0,  # ê¸°ë³¸ê°’
            1,  # ì–‘ìˆ˜
            -1,  # ìŒìˆ˜
            min_val,  # ìµœì†Ÿê°’
            max_val,  # ìµœëŒ“ê°’
        ]
        
        # ê²½ê³„ê°’
        if min_val != -1000:
            data.extend([min_val - 1, min_val + 1])
        if max_val != 1000:
            data.extend([max_val - 1, max_val + 1])
        
        # ëœë¤ê°’
        for _ in range(3):
            data.append(self.random.randint(min_val, max_val))
        
        return list(set(data))
    
    def _generate_float_data(self, constraints: Dict) -> List[float]:
        """ì‹¤ìˆ˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        min_val = constraints.get("min", -1000.0)
        max_val = constraints.get("max", 1000.0)
        
        data = [
            0.0,  # ê¸°ë³¸ê°’
            1.0,  # ì–‘ìˆ˜
            -1.0,  # ìŒìˆ˜
            0.1,  # ì‘ì€ ì–‘ìˆ˜
            -0.1,  # ì‘ì€ ìŒìˆ˜
            min_val,  # ìµœì†Ÿê°’
            max_val,  # ìµœëŒ“ê°’
            float('inf'),  # ë¬´í•œëŒ€
            float('-inf'),  # ìŒì˜ ë¬´í•œëŒ€
            float('nan'),  # NaN
        ]
        
        # ëœë¤ê°’
        for _ in range(3):
            data.append(self.random.uniform(min_val, max_val))
        
        return data
    
    def _generate_string_data(self, constraints: Dict) -> List[str]:
        """ë¬¸ìì—´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        min_len = constraints.get("min_length", 0)
        max_len = constraints.get("max_length", 100)
        
        data = [
            "",  # ë¹ˆ ë¬¸ìì—´
            " ",  # ê³µë°±
            "a",  # ë‹¨ì¼ ë¬¸ì
            "test",  # ì¼ë°˜ ë¬¸ìì—´
            "ê°€ë‚˜ë‹¤",  # í•œê¸€
            "ğŸš€ğŸ¯",  # ì´ëª¨ì§€
            "null",  # íŠ¹ìˆ˜ ë¬¸ìì—´
            "None",
            "\n\t",  # ì œì–´ ë¬¸ì
            "' OR '1'='1",  # SQL ì¸ì ì…˜ í…ŒìŠ¤íŠ¸
            "<script>alert('xss')</script>",  # XSS í…ŒìŠ¤íŠ¸
        ]
        
        # ê¸¸ì´ ê¸°ë°˜ í…ŒìŠ¤íŠ¸
        if min_len > 0:
            data.append("a" * (min_len - 1))  # ìµœì†Œê¸¸ì´ë³´ë‹¤ ì§§ìŒ
            data.append("a" * min_len)  # ìµœì†Œê¸¸ì´
        
        if max_len < 100:
            data.append("a" * max_len)  # ìµœëŒ€ê¸¸ì´
            data.append("a" * (max_len + 1))  # ìµœëŒ€ê¸¸ì´ë³´ë‹¤ ê¹€
        
        # ê¸´ ë¬¸ìì—´
        data.append("a" * 1000)
        
        return data
    
    def _generate_bool_data(self) -> List[bool]:
        """ë¶ˆë¦° í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        return [True, False]
    
    def _generate_list_data(self, constraints: Dict) -> List[List]:
        """ë¦¬ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        element_type = constraints.get("element_type", "int")
        min_len = constraints.get("min_length", 0)
        max_len = constraints.get("max_length", 10)
        
        data = [
            [],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
            [1],  # ë‹¨ì¼ ìš”ì†Œ
            [1, 2, 3],  # ì¼ë°˜ ë¦¬ìŠ¤íŠ¸
            [1, 1, 1],  # ì¤‘ë³µ ìš”ì†Œ
        ]
        
        # ê¸¸ì´ ê¸°ë°˜ í…ŒìŠ¤íŠ¸
        if min_len > 0:
            data.append([1] * min_len)
        if max_len > 3:
            data.append([1] * max_len)
        
        # í° ë¦¬ìŠ¤íŠ¸
        data.append(list(range(1000)))
        
        return data
    
    def _generate_dict_data(self, constraints: Dict) -> List[Dict]:
        """ë”•ì…”ë„ˆë¦¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        return [
            {},  # ë¹ˆ ë”•ì…”ë„ˆë¦¬
            {"key": "value"},  # ë‹¨ì¼ í‚¤-ê°’
            {"a": 1, "b": 2},  # ë‹¤ì¤‘ í‚¤-ê°’
            {"nested": {"key": "value"}},  # ì¤‘ì²© ë”•ì…”ë„ˆë¦¬
            {1: "number_key"},  # ìˆ«ì í‚¤
            {"": "empty_key"},  # ë¹ˆ í‚¤
        ]
    
    def _generate_tuple_data(self, constraints: Dict) -> List[Tuple]:
        """íŠœí”Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        return [
            (),  # ë¹ˆ íŠœí”Œ
            (1,),  # ë‹¨ì¼ ìš”ì†Œ
            (1, 2, 3),  # ë‹¤ì¤‘ ìš”ì†Œ
            (1, "mixed", True),  # í˜¼í•© íƒ€ì…
        ]
    
    def _generate_set_data(self, constraints: Dict) -> List[Set]:
        """ì§‘í•© í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        return [
            set(),  # ë¹ˆ ì§‘í•©
            {1},  # ë‹¨ì¼ ìš”ì†Œ
            {1, 2, 3},  # ë‹¤ì¤‘ ìš”ì†Œ
        ]
    
    def _generate_custom_data(self, param_type: str, constraints: Dict) -> List[Any]:
        """ì»¤ìŠ¤í…€ íƒ€ì… ë°ì´í„° ìƒì„±"""
        # ê¸°ë³¸ì ìœ¼ë¡œ Noneê³¼ ë¹ˆ ê°’ë“¤ ë°˜í™˜
        return [None, "", 0, [], {}]
    
    def generate_edge_cases(self, function_analysis: Dict[str, Any]) -> List[EdgeCase]:
        """ì—£ì§€ ì¼€ì´ìŠ¤ ìƒì„±"""
        edge_cases = []
        
        # ë§¤ê°œë³€ìˆ˜ ê¸°ë°˜ ì—£ì§€ ì¼€ì´ìŠ¤
        for param in function_analysis.get("parameters", []):
            param_name = param["name"]
            param_type = param.get("annotation", "Any")
            
            # None ê°’ í…ŒìŠ¤íŠ¸
            edge_cases.append(EdgeCase(
                name=f"{param_name}_none",
                description=f"{param_name}ì— None ê°’ ì „ë‹¬",
                input_values={param_name: None},
                expected_behavior="ì ì ˆí•œ ì˜ˆì™¸ ì²˜ë¦¬ ë˜ëŠ” ê¸°ë³¸ê°’ ì²˜ë¦¬",
                category="null"
            ))
            
            # íƒ€ì…ë³„ ê²½ê³„ê°’ í…ŒìŠ¤íŠ¸
            if "int" in param_type:
                edge_cases.extend([
                    EdgeCase(
                        name=f"{param_name}_max_int",
                        description=f"{param_name}ì— ìµœëŒ€ ì •ìˆ˜ê°’ ì „ë‹¬",
                        input_values={param_name: sys.maxsize},
                        expected_behavior="ì •ìƒ ì²˜ë¦¬ ë˜ëŠ” ì˜¤ë²„í”Œë¡œìš° ì˜ˆì™¸",
                        category="boundary"
                    ),
                    EdgeCase(
                        name=f"{param_name}_min_int",
                        description=f"{param_name}ì— ìµœì†Œ ì •ìˆ˜ê°’ ì „ë‹¬",
                        input_values={param_name: -sys.maxsize - 1},
                        expected_behavior="ì •ìƒ ì²˜ë¦¬ ë˜ëŠ” ì–¸ë”í”Œë¡œìš° ì˜ˆì™¸",
                        category="boundary"
                    )
                ])
            
            elif "str" in param_type:
                edge_cases.extend([
                    EdgeCase(
                        name=f"{param_name}_empty_string",
                        description=f"{param_name}ì— ë¹ˆ ë¬¸ìì—´ ì „ë‹¬",
                        input_values={param_name: ""},
                        expected_behavior="ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬",
                        category="boundary"
                    ),
                    EdgeCase(
                        name=f"{param_name}_very_long_string",
                        description=f"{param_name}ì— ë§¤ìš° ê¸´ ë¬¸ìì—´ ì „ë‹¬",
                        input_values={param_name: "a" * 10000},
                        expected_behavior="ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬",
                        category="performance"
                    )
                ])
        
        return edge_cases


class UnitTestGenerator:
    """ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self, framework: TestFramework = TestFramework.PYTEST):
        self.framework = framework
        self.function_analyzer = FunctionAnalyzer()
        self.data_generator = TestDataGenerator()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def generate_tests(self, function_code: str, additional_context: Optional[Dict] = None) -> List[TestCase]:
        """í•¨ìˆ˜ì— ëŒ€í•œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ìƒì„±"""
        try:
            # í•¨ìˆ˜ ë¶„ì„
            analysis = self.function_analyzer.analyze_function(function_code)
            if "error" in analysis:
                raise TestGenerationError(f"í•¨ìˆ˜ ë¶„ì„ ì‹¤íŒ¨: {analysis['error']}")
            
            function_name = analysis["name"]
            if not function_name:
                raise TestGenerationError("í•¨ìˆ˜ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            test_cases = []
            
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
            test_cases.extend(self._generate_basic_tests(analysis))
            
            # ë§¤ê°œë³€ìˆ˜ í…ŒìŠ¤íŠ¸
            test_cases.extend(self._generate_parameter_tests(analysis))
            
            # ì˜ˆì™¸ í…ŒìŠ¤íŠ¸
            test_cases.extend(self._generate_exception_tests(analysis))
            
            # ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸
            test_cases.extend(self._generate_edge_case_tests(analysis))
            
            # ëª¨í‚¹ì´ í•„ìš”í•œ í…ŒìŠ¤íŠ¸
            if analysis.get("dependencies"):
                test_cases.extend(self._generate_mock_tests(analysis))
            
            self.logger.info(f"{function_name}ì— ëŒ€í•´ {len(test_cases)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±")
            
            return test_cases
            
        except Exception as e:
            self.logger.error(f"í…ŒìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise TestGenerationError(f"í…ŒìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _generate_basic_tests(self, analysis: Dict[str, Any]) -> List[TestCase]:
        """ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±"""
        function_name = analysis["name"]
        parameters = analysis["parameters"]
        
        test_cases = []
        
        # ì •ìƒ ë™ì‘ í…ŒìŠ¤íŠ¸
        if parameters:
            # ë§¤ê°œë³€ìˆ˜ê°€ ìˆëŠ” ê²½ìš°
            param_calls = []
            for param in parameters:
                param_name = param["name"]
                param_type = param.get("annotation", "Any")
                default_value = param.get("default")
                
                if default_value is not None:
                    param_calls.append(f"{param_name}={repr(default_value)}")
                else:
                    # íƒ€ì…ì— ë”°ë¥¸ ê¸°ë³¸ê°’ ìƒì„±
                    test_value = self._get_default_test_value(param_type)
                    param_calls.append(f"{param_name}={repr(test_value)}")
            
            test_code = self._generate_test_method(
                f"test_{function_name}_basic",
                function_name,
                param_calls,
                "ì •ìƒì ì¸ ì…ë ¥ìœ¼ë¡œ í•¨ìˆ˜ í˜¸ì¶œ í…ŒìŠ¤íŠ¸"
            )
        else:
            # ë§¤ê°œë³€ìˆ˜ê°€ ì—†ëŠ” ê²½ìš°
            test_code = self._generate_test_method(
                f"test_{function_name}_no_params",
                function_name,
                [],
                "ë§¤ê°œë³€ìˆ˜ ì—†ì´ í•¨ìˆ˜ í˜¸ì¶œ í…ŒìŠ¤íŠ¸"
            )
        
        test_cases.append(TestCase(
            name=f"test_{function_name}_basic",
            test_type=TestType.UNIT,
            function_name=function_name,
            test_code=test_code,
            description="ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸",
            priority=TestPriority.HIGH
        ))
        
        return test_cases
    
    def _generate_parameter_tests(self, analysis: Dict[str, Any]) -> List[TestCase]:
        """ë§¤ê°œë³€ìˆ˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±"""
        function_name = analysis["name"]
        parameters = analysis["parameters"]
        
        test_cases = []
        
        for param in parameters:
            param_name = param["name"]
            param_type = param.get("annotation", "Any")
            
            # íƒ€ì…ë³„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            test_data = self.data_generator.generate_test_data(param_type)
            
            for i, test_value in enumerate(test_data[:5]):  # ìµœëŒ€ 5ê°œê¹Œì§€
                param_calls = [f"{param_name}={repr(test_value)}"]
                
                # ë‹¤ë¥¸ ë§¤ê°œë³€ìˆ˜ë“¤ì€ ê¸°ë³¸ê°’ ì‚¬ìš©
                for other_param in parameters:
                    if other_param["name"] != param_name:
                        default_val = self._get_default_test_value(other_param.get("annotation", "Any"))
                        param_calls.append(f"{other_param['name']}={repr(default_val)}")
                
                test_method_name = f"test_{function_name}_{param_name}_type_{i}"
                test_code = self._generate_test_method(
                    test_method_name,
                    function_name,
                    param_calls,
                    f"{param_name} ë§¤ê°œë³€ìˆ˜ íƒ€ì… í…ŒìŠ¤íŠ¸: {test_value}"
                )
                
                test_cases.append(TestCase(
                    name=test_method_name,
                    test_type=TestType.UNIT,
                    function_name=function_name,
                    test_code=test_code,
                    description=f"{param_name} ë§¤ê°œë³€ìˆ˜ í…ŒìŠ¤íŠ¸",
                    priority=TestPriority.MEDIUM,
                    tags=[f"param_{param_name}", "type_test"]
                ))
        
        return test_cases
    
    def _generate_exception_tests(self, analysis: Dict[str, Any]) -> List[TestCase]:
        """ì˜ˆì™¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±"""
        function_name = analysis["name"]
        parameters = analysis["parameters"]
        expected_exceptions = analysis.get("exceptions", [])
        
        test_cases = []
        
        # ëª…ì‹œì  ì˜ˆì™¸ í…ŒìŠ¤íŠ¸
        for exception in expected_exceptions:
            test_method_name = f"test_{function_name}_{exception.lower()}_exception"
            
            # ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ì…ë ¥ ìƒì„±
            param_calls = []
            for param in parameters:
                param_name = param["name"]
                # ì˜ˆì™¸ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆëŠ” ê°’ ì‚¬ìš©
                bad_value = self._get_exception_causing_value(param_name, exception)
                param_calls.append(f"{param_name}={repr(bad_value)}")
            
            test_code = self._generate_exception_test_method(
                test_method_name,
                function_name,
                param_calls,
                exception,
                f"{exception} ì˜ˆì™¸ ë°œìƒ í…ŒìŠ¤íŠ¸"
            )
            
            test_cases.append(TestCase(
                name=test_method_name,
                test_type=TestType.UNIT,
                function_name=function_name,
                test_code=test_code,
                description=f"{exception} ì˜ˆì™¸ í…ŒìŠ¤íŠ¸",
                priority=TestPriority.HIGH,
                tags=["exception_test", exception.lower()]
            ))
        
        # ì¼ë°˜ì ì¸ ì˜ˆì™¸ í…ŒìŠ¤íŠ¸
        if parameters:
            # TypeError í…ŒìŠ¤íŠ¸ (ì˜ëª»ëœ íƒ€ì…)
            param_calls = []
            for param in parameters:
                param_name = param["name"]
                param_type = param.get("annotation", "Any")
                wrong_type_value = self._get_wrong_type_value(param_type)
                param_calls.append(f"{param_name}={repr(wrong_type_value)}")
            
            test_code = self._generate_exception_test_method(
                f"test_{function_name}_type_error",
                function_name,
                param_calls,
                "TypeError",
                "ì˜ëª»ëœ íƒ€ì… ì…ë ¥ ì‹œ TypeError ë°œìƒ í…ŒìŠ¤íŠ¸"
            )
            
            test_cases.append(TestCase(
                name=f"test_{function_name}_type_error",
                test_type=TestType.UNIT,
                function_name=function_name,
                test_code=test_code,
                description="TypeError í…ŒìŠ¤íŠ¸",
                priority=TestPriority.MEDIUM,
                tags=["exception_test", "type_error"]
            ))
        
        return test_cases
    
    def _generate_edge_case_tests(self, analysis: Dict[str, Any]) -> List[TestCase]:
        """ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ìƒì„±"""
        function_name = analysis["name"]
        edge_cases = self.data_generator.generate_edge_cases(analysis)
        
        test_cases = []
        
        for edge_case in edge_cases:
            param_calls = []
            for param_name, value in edge_case.input_values.items():
                param_calls.append(f"{param_name}={repr(value)}")
            
            test_method_name = f"test_{function_name}_{edge_case.name}"
            test_code = self._generate_test_method(
                test_method_name,
                function_name,
                param_calls,
                edge_case.description
            )
            
            test_cases.append(TestCase(
                name=test_method_name,
                test_type=TestType.UNIT,
                function_name=function_name,
                test_code=test_code,
                description=edge_case.description,
                priority=edge_case.severity,
                tags=["edge_case", edge_case.category],
                edge_cases=[edge_case.name]
            ))
        
        return test_cases
    
    def _generate_mock_tests(self, analysis: Dict[str, Any]) -> List[TestCase]:
        """ëª¨í‚¹ì´ í•„ìš”í•œ í…ŒìŠ¤íŠ¸ ìƒì„±"""
        function_name = analysis["name"]
        dependencies = analysis.get("dependencies", [])
        
        test_cases = []
        
        for dependency in dependencies:
            # ì™¸ë¶€ ì˜ì¡´ì„±ì„ ëª¨í‚¹í•˜ëŠ” í…ŒìŠ¤íŠ¸
            test_method_name = f"test_{function_name}_mock_{dependency.replace('.', '_')}"
            
            mock_spec = MockSpec(
                target=dependency,
                mock_type="function",
                return_value="mocked_result"
            )
            
            test_code = self._generate_mock_test_method(
                test_method_name,
                function_name,
                analysis["parameters"],
                mock_spec,
                f"{dependency} ì˜ì¡´ì„± ëª¨í‚¹ í…ŒìŠ¤íŠ¸"
            )
            
            test_cases.append(TestCase(
                name=test_method_name,
                test_type=TestType.UNIT,
                function_name=function_name,
                test_code=test_code,
                description=f"{dependency} ëª¨í‚¹ í…ŒìŠ¤íŠ¸",
                priority=TestPriority.MEDIUM,
                tags=["mock_test", "dependency"],
                mock_requirements=[dependency]
            ))
        
        return test_cases
    
    def _generate_test_method(self, method_name: str, function_name: str, 
                            param_calls: List[str], description: str) -> str:
        """í…ŒìŠ¤íŠ¸ ë©”ì†Œë“œ ì½”ë“œ ìƒì„±"""
        if self.framework == TestFramework.PYTEST:
            return self._generate_pytest_method(method_name, function_name, param_calls, description)
        elif self.framework == TestFramework.UNITTEST:
            return self._generate_unittest_method(method_name, function_name, param_calls, description)
        else:
            return self._generate_pytest_method(method_name, function_name, param_calls, description)
    
    def _generate_pytest_method(self, method_name: str, function_name: str, 
                               param_calls: List[str], description: str) -> str:
        """pytest ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸ ë©”ì†Œë“œ ìƒì„±"""
        params_str = ", ".join(param_calls) if param_calls else ""
        
        return f'''def {method_name}():
    """
    {description}
    """
    # Arrange
    # TODO: í•„ìš”í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    
    # Act
    result = {function_name}({params_str})
    
    # Assert
    assert result is not None  # TODO: ì ì ˆí•œ ê²€ì¦ ë¡œì§ ì¶”ê°€
    # TODO: ì˜ˆìƒ ê²°ê³¼ì™€ ë¹„êµ
'''
    
    def _generate_unittest_method(self, method_name: str, function_name: str, 
                                param_calls: List[str], description: str) -> str:
        """unittest ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸ ë©”ì†Œë“œ ìƒì„±"""
        params_str = ", ".join(param_calls) if param_calls else ""
        
        return f'''def {method_name}(self):
    """
    {description}
    """
    # Arrange
    # TODO: í•„ìš”í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    
    # Act
    result = {function_name}({params_str})
    
    # Assert
    self.assertIsNotNone(result)  # TODO: ì ì ˆí•œ ê²€ì¦ ë¡œì§ ì¶”ê°€
    # TODO: ì˜ˆìƒ ê²°ê³¼ì™€ ë¹„êµ
'''
    
    def _generate_exception_test_method(self, method_name: str, function_name: str,
                                      param_calls: List[str], exception_type: str,
                                      description: str) -> str:
        """ì˜ˆì™¸ í…ŒìŠ¤íŠ¸ ë©”ì†Œë“œ ìƒì„±"""
        params_str = ", ".join(param_calls) if param_calls else ""
        
        if self.framework == TestFramework.PYTEST:
            return f'''def {method_name}():
    """
    {description}
    """
    import pytest
    
    # Arrange & Act & Assert
    with pytest.raises({exception_type}):
        {function_name}({params_str})
'''
        else:
            return f'''def {method_name}(self):
    """
    {description}
    """
    # Arrange & Act & Assert
    with self.assertRaises({exception_type}):
        {function_name}({params_str})
'''
    
    def _generate_mock_test_method(self, method_name: str, function_name: str,
                                 parameters: List[Dict], mock_spec: MockSpec,
                                 description: str) -> str:
        """ëª¨í‚¹ í…ŒìŠ¤íŠ¸ ë©”ì†Œë“œ ìƒì„±"""
        param_calls = []
        for param in parameters:
            default_val = self._get_default_test_value(param.get("annotation", "Any"))
            param_calls.append(f"{param['name']}={repr(default_val)}")
        
        params_str = ", ".join(param_calls) if param_calls else ""
        
        if self.framework == TestFramework.PYTEST:
            return f'''def {method_name}(mocker):
    """
    {description}
    """
    # Arrange
    mock_{mock_spec.target.replace('.', '_')} = mocker.patch('{mock_spec.target}')
    mock_{mock_spec.target.replace('.', '_')}.return_value = {repr(mock_spec.return_value)}
    
    # Act
    result = {function_name}({params_str})
    
    # Assert
    assert result is not None
    mock_{mock_spec.target.replace('.', '_')}.assert_called_once()
'''
        else:
            return f'''@patch('{mock_spec.target}')
def {method_name}(self, mock_{mock_spec.target.replace('.', '_')}):
    """
    {description}
    """
    # Arrange
    mock_{mock_spec.target.replace('.', '_')}.return_value = {repr(mock_spec.return_value)}
    
    # Act
    result = {function_name}({params_str})
    
    # Assert
    self.assertIsNotNone(result)
    mock_{mock_spec.target.replace('.', '_')}.assert_called_once()
'''
    
    def _get_default_test_value(self, param_type: str) -> Any:
        """ë§¤ê°œë³€ìˆ˜ íƒ€ì…ì— ë”°ë¥¸ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ê°’ ë°˜í™˜"""
        type_defaults = {
            "int": 1,
            "float": 1.0,
            "str": "test",
            "bool": True,
            "list": [1, 2, 3],
            "dict": {"key": "value"},
            "tuple": (1, 2, 3),
            "set": {1, 2, 3},
        }
        
        for type_name, default_val in type_defaults.items():
            if type_name in param_type.lower():
                return default_val
        
        return "test_value"
    
    def _get_exception_causing_value(self, param_name: str, exception_type: str) -> Any:
        """ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ê°’ ë°˜í™˜"""
        if exception_type == "ValueError":
            return "invalid_value"
        elif exception_type == "TypeError":
            return None
        elif exception_type == "ZeroDivisionError":
            return 0
        elif exception_type == "IndexError":
            return -1
        else:
            return None
    
    def _get_wrong_type_value(self, expected_type: str) -> Any:
        """ì˜ëª»ëœ íƒ€ì… ê°’ ë°˜í™˜"""
        if "int" in expected_type:
            return "not_an_int"
        elif "str" in expected_type:
            return 123
        elif "list" in expected_type:
            return "not_a_list"
        elif "dict" in expected_type:
            return "not_a_dict"
        else:
            return object()


class IntegrationTestGenerator:
    """í†µí•© í…ŒìŠ¤íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def generate_integration_tests(self, modules: List[str], 
                                 interactions: List[Dict[str, Any]]) -> List[TestCase]:
        """ëª¨ë“ˆ ê°„ ìƒí˜¸ì‘ìš© í…ŒìŠ¤íŠ¸ ìƒì„±"""
        test_cases = []
        
        for interaction in interactions:
            source_module = interaction.get("source")
            target_module = interaction.get("target")
            interaction_type = interaction.get("type", "call")
            
            test_case = self._generate_module_interaction_test(
                source_module, target_module, interaction_type
            )
            test_cases.append(test_case)
        
        return test_cases
    
    def _generate_module_interaction_test(self, source: str, target: str, 
                                        interaction_type: str) -> TestCase:
        """ëª¨ë“ˆ ê°„ ìƒí˜¸ì‘ìš© í…ŒìŠ¤íŠ¸ ìƒì„±"""
        test_name = f"test_integration_{source}_{target}_{interaction_type}"
        
        test_code = f'''def {test_name}():
    """
    {source}ì™€ {target} ëª¨ë“ˆ ê°„ {interaction_type} ìƒí˜¸ì‘ìš© í…ŒìŠ¤íŠ¸
    """
    # Arrange
    # TODO: í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
    
    # Act
    # TODO: {source}ì—ì„œ {target}ë¡œì˜ {interaction_type} ì‹¤í–‰
    
    # Assert
    # TODO: ìƒí˜¸ì‘ìš© ê²°ê³¼ ê²€ì¦
    pass
'''
        
        return TestCase(
            name=test_name,
            test_type=TestType.INTEGRATION,
            function_name=f"{source}_{target}",
            test_code=test_code,
            description=f"{source}ì™€ {target} ëª¨ë“ˆ í†µí•© í…ŒìŠ¤íŠ¸",
            priority=TestPriority.HIGH,
            tags=["integration", source, target],
            dependencies=[source, target]
        )


class CoverageAnalyzer:
    """ì»¤ë²„ë¦¬ì§€ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def analyze_coverage(self, source_files: List[str], test_files: List[str], 
                        coverage_types: List[CoverageType] = None) -> CoverageReport:
        """ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì‹¤í–‰"""
        if coverage_types is None:
            coverage_types = [CoverageType.LINE, CoverageType.BRANCH, CoverageType.FUNCTION]
        
        try:
            # coverage.pyë¥¼ ì‚¬ìš©í•œ ë¶„ì„
            coverage_data = self._run_coverage_analysis(source_files, test_files)
            
            report = CoverageReport(
                total_lines=coverage_data.get("total_lines", 0),
                covered_lines=coverage_data.get("covered_lines", 0),
                line_coverage=coverage_data.get("line_coverage", 0.0),
                branch_coverage=coverage_data.get("branch_coverage", 0.0),
                function_coverage=coverage_data.get("function_coverage", 0.0),
                missing_lines=coverage_data.get("missing_lines", []),
                uncovered_branches=coverage_data.get("uncovered_branches", []),
                uncovered_functions=coverage_data.get("uncovered_functions", []),
                file_coverage=coverage_data.get("file_coverage", {})
            )
            
            return report
            
        except Exception as e:
            self.logger.error(f"ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise CoverageAnalysisError(f"ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _run_coverage_analysis(self, source_files: List[str], test_files: List[str]) -> Dict[str, Any]:
        """ì‹¤ì œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì‹¤í–‰"""
        try:
            # coverage.py ì‹¤í–‰
            import coverage
            
            cov = coverage.Coverage(source=source_files)
            cov.start()
            
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì‹¤ì œë¡œëŠ” pytestë‚˜ unittest ì‹¤í–‰)
            for test_file in test_files:
                self._run_test_file(test_file)
            
            cov.stop()
            cov.save()
            
            # ê²°ê³¼ ë¶„ì„
            total_lines = 0
            covered_lines = 0
            missing_lines = []
            file_coverage = {}
            
            for filename in source_files:
                analysis = cov.analysis2(filename)
                if analysis:
                    statements, excluded, missing, missed_branches = analysis
                    
                    file_total = len(statements)
                    file_covered = file_total - len(missing)
                    
                    total_lines += file_total
                    covered_lines += file_covered
                    missing_lines.extend(missing)
                    
                    file_coverage[filename] = file_covered / file_total if file_total > 0 else 0.0
            
            line_coverage = covered_lines / total_lines if total_lines > 0 else 0.0
            
            return {
                "total_lines": total_lines,
                "covered_lines": covered_lines,
                "line_coverage": line_coverage,
                "branch_coverage": 0.0,  # ì¶”í›„ êµ¬í˜„
                "function_coverage": 0.0,  # ì¶”í›„ êµ¬í˜„
                "missing_lines": missing_lines,
                "uncovered_branches": [],
                "uncovered_functions": [],
                "file_coverage": file_coverage
            }
            
        except ImportError:
            # coverage.pyê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ë¶„ì„
            return self._basic_coverage_analysis(source_files, test_files)
        except Exception as e:
            self.logger.error(f"Coverage ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return self._basic_coverage_analysis(source_files, test_files)
    
    def _basic_coverage_analysis(self, source_files: List[str], test_files: List[str]) -> Dict[str, Any]:
        """ê¸°ë³¸ ì»¤ë²„ë¦¬ì§€ ë¶„ì„ (coverage.py ì—†ì´)"""
        total_lines = 0
        file_coverage = {}
        
        for source_file in source_files:
            try:
                with open(source_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    executable_lines = [i for i, line in enumerate(lines, 1) 
                                      if line.strip() and not line.strip().startswith('#')]
                    total_lines += len(executable_lines)
                    
                    # ê¸°ë³¸ì ìœ¼ë¡œ 50% ì»¤ë²„ë¦¬ì§€ ê°€ì •
                    file_coverage[source_file] = 0.5
            except Exception as e:
                self.logger.warning(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {source_file}, {e}")
        
        covered_lines = int(total_lines * 0.5)
        
        return {
            "total_lines": total_lines,
            "covered_lines": covered_lines,
            "line_coverage": 0.5,
            "branch_coverage": 0.4,
            "function_coverage": 0.6,
            "missing_lines": list(range(covered_lines + 1, total_lines + 1)),
            "uncovered_branches": [],
            "uncovered_functions": [],
            "file_coverage": file_coverage
        }
    
    def _run_test_file(self, test_file: str):
        """í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‹¤í–‰"""
        try:
            # ë™ì  ëª¨ë“ˆ ì„í¬íŠ¸ ë° ì‹¤í–‰
            spec = importlib.util.spec_from_file_location("test_module", test_file)
            if spec and spec.loader:
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
        except Exception as e:
            self.logger.warning(f"í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‹¤í–‰ ì‹¤íŒ¨: {test_file}, {e}")
    
    def suggest_coverage_improvements(self, report: CoverageReport) -> List[str]:
        """ì»¤ë²„ë¦¬ì§€ ê°œì„  ì œì•ˆ"""
        suggestions = []
        
        if report.line_coverage < 0.8:
            suggestions.append(f"ë¼ì¸ ì»¤ë²„ë¦¬ì§€ê°€ {report.line_coverage:.1%}ë¡œ ë‚®ìŠµë‹ˆë‹¤. 80% ì´ìƒì„ ëª©í‘œë¡œ í•˜ì„¸ìš”.")
        
        if report.branch_coverage < 0.7:
            suggestions.append(f"ë¸Œëœì¹˜ ì»¤ë²„ë¦¬ì§€ê°€ {report.branch_coverage:.1%}ë¡œ ë‚®ìŠµë‹ˆë‹¤. ì¡°ê±´ë¬¸ í…ŒìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        
        if report.function_coverage < 0.9:
            suggestions.append(f"í•¨ìˆ˜ ì»¤ë²„ë¦¬ì§€ê°€ {report.function_coverage:.1%}ë¡œ ë‚®ìŠµë‹ˆë‹¤. ëª¨ë“  í•¨ìˆ˜ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        
        # íŒŒì¼ë³„ ê°œì„  ì œì•ˆ
        low_coverage_files = [
            file for file, coverage in report.file_coverage.items() 
            if coverage < 0.7
        ]
        
        if low_coverage_files:
            suggestions.append(f"ë‹¤ìŒ íŒŒì¼ë“¤ì˜ ì»¤ë²„ë¦¬ì§€ê°€ ë‚®ìŠµë‹ˆë‹¤: {', '.join(low_coverage_files[:3])}")
        
        # ëˆ„ë½ëœ ë¼ì¸ ì •ë³´
        if report.missing_lines:
            suggestions.append(f"{len(report.missing_lines)}ê°œì˜ ë¼ì¸ì´ í…ŒìŠ¤íŠ¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return suggestions


class TestExecutor:
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self, framework: TestFramework = TestFramework.PYTEST):
        self.framework = framework
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def execute_tests(self, test_suite: TestSuite, 
                     parallel: bool = False, 
                     verbose: bool = False) -> List[TestResult]:
        """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
        results = []
        
        try:
            if parallel and test_suite.parallel_execution:
                results = self._execute_parallel(test_suite, verbose)
            else:
                results = self._execute_sequential(test_suite, verbose)
            
            return results
            
        except Exception as e:
            self.logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise TestExecutionError(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    def _execute_sequential(self, test_suite: TestSuite, verbose: bool) -> List[TestResult]:
        """ìˆœì°¨ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        results = []
        
        for test_case in test_suite.test_cases:
            result = self._execute_single_test(test_case, verbose)
            results.append(result)
        
        return results
    
    def _execute_parallel(self, test_suite: TestSuite, verbose: bool) -> List[TestResult]:
        """ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        results = []
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_test = {
                executor.submit(self._execute_single_test, test_case, verbose): test_case
                for test_case in test_suite.test_cases
            }
            
            for future in as_completed(future_to_test):
                test_case = future_to_test[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    self.logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {test_case.name}, {e}")
                    results.append(TestResult(
                        test_name=test_case.name,
                        status="error",
                        execution_time=0.0,
                        error_message=str(e)
                    ))
        
        return results
    
    def _execute_single_test(self, test_case: TestCase, verbose: bool) -> TestResult:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            # í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì‹¤í–‰
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(test_case.test_code)
                temp_file = f.name
            
            try:
                # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                if self.framework == TestFramework.PYTEST:
                    result = self._run_pytest(temp_file, test_case.name, verbose)
                else:
                    result = self._run_unittest(temp_file, test_case.name, verbose)
                
                execution_time = time.time() - start_time
                
                return TestResult(
                    test_name=test_case.name,
                    status=result["status"],
                    execution_time=execution_time,
                    error_message=result.get("error"),
                    traceback=result.get("traceback"),
                    assertions_count=result.get("assertions", 0)
                )
                
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                try:
                    Path(temp_file).unlink()
                except Exception:
                    pass
                
        except Exception as e:
            execution_time = time.time() - start_time
            return TestResult(
                test_name=test_case.name,
                status="error",
                execution_time=execution_time,
                error_message=str(e),
                traceback=traceback.format_exc()
            )
    
    def _run_pytest(self, test_file: str, test_name: str, verbose: bool) -> Dict[str, Any]:
        """pytest ì‹¤í–‰"""
        try:
            cmd = ["python", "-m", "pytest", test_file, "-v" if verbose else "-q"]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                return {"status": "passed"}
            else:
                return {
                    "status": "failed",
                    "error": result.stdout + result.stderr,
                    "traceback": result.stderr
                }
                
        except subprocess.TimeoutExpired:
            return {"status": "timeout", "error": "Test execution timed out"}
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    def _run_unittest(self, test_file: str, test_name: str, verbose: bool) -> Dict[str, Any]:
        """unittest ì‹¤í–‰"""
        try:
            cmd = ["python", "-m", "unittest", test_file, "-v" if verbose else ""]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                return {"status": "passed"}
            else:
                return {
                    "status": "failed",
                    "error": result.stdout + result.stderr,
                    "traceback": result.stderr
                }
                
        except subprocess.TimeoutExpired:
            return {"status": "timeout", "error": "Test execution timed out"}
        except Exception as e:
            return {"status": "error", "error": str(e)}


class TestQualityAnalyzer:
    """í…ŒìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def analyze_test_quality(self, test_cases: List[TestCase]) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„"""
        if not test_cases:
            return {"error": "ë¶„ì„í•  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        analysis = {
            "total_tests": len(test_cases),
            "test_types": self._analyze_test_types(test_cases),
            "priority_distribution": self._analyze_priority_distribution(test_cases),
            "coverage_potential": self._estimate_coverage_potential(test_cases),
            "quality_score": 0.0,
            "recommendations": []
        }
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        analysis["quality_score"] = self._calculate_quality_score(analysis)
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        analysis["recommendations"] = self._generate_quality_recommendations(analysis)
        
        return analysis
    
    def _analyze_test_types(self, test_cases: List[TestCase]) -> Dict[str, int]:
        """í…ŒìŠ¤íŠ¸ íƒ€ì…ë³„ ë¶„í¬ ë¶„ì„"""
        type_counts = {}
        for test_case in test_cases:
            test_type = test_case.test_type.value
            type_counts[test_type] = type_counts.get(test_type, 0) + 1
        return type_counts
    
    def _analyze_priority_distribution(self, test_cases: List[TestCase]) -> Dict[str, int]:
        """ìš°ì„ ìˆœìœ„ë³„ ë¶„í¬ ë¶„ì„"""
        priority_counts = {}
        for test_case in test_cases:
            priority = test_case.priority.value
            priority_counts[priority] = priority_counts.get(priority, 0) + 1
        return priority_counts
    
    def _estimate_coverage_potential(self, test_cases: List[TestCase]) -> float:
        """ì»¤ë²„ë¦¬ì§€ ì ì¬ë ¥ ì¶”ì •"""
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜ì™€ ë‹¤ì–‘ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •
        base_score = min(len(test_cases) / 10.0, 1.0)  # 10ê°œë‹¹ 100%
        
        # ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
        unique_functions = len(set(tc.function_name for tc in test_cases))
        diversity_bonus = min(unique_functions / 5.0, 0.3)  # ìµœëŒ€ 30% ë³´ë„ˆìŠ¤
        
        return min(base_score + diversity_bonus, 1.0)
    
    def _calculate_quality_score(self, analysis: Dict[str, Any]) -> float:
        """í…ŒìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
        score = 0.0
        
        # í…ŒìŠ¤íŠ¸ ìˆ˜ëŸ‰ ì ìˆ˜ (30ì  ë§Œì )
        test_count = analysis["total_tests"]
        quantity_score = min(test_count / 20.0 * 30, 30)  # 20ê°œë©´ ë§Œì 
        score += quantity_score
        
        # íƒ€ì… ë‹¤ì–‘ì„± ì ìˆ˜ (25ì  ë§Œì )
        type_diversity = len(analysis["test_types"])
        diversity_score = min(type_diversity / 3.0 * 25, 25)  # 3ê°€ì§€ íƒ€ì…ì´ë©´ ë§Œì 
        score += diversity_score
        
        # ìš°ì„ ìˆœìœ„ ê· í˜• ì ìˆ˜ (20ì  ë§Œì )
        priorities = analysis["priority_distribution"]
        if len(priorities) >= 3:  # 3ê°€ì§€ ì´ìƒ ìš°ì„ ìˆœìœ„
            balance_score = 20
        elif len(priorities) == 2:
            balance_score = 15
        else:
            balance_score = 10
        score += balance_score
        
        # ì»¤ë²„ë¦¬ì§€ ì ì¬ë ¥ ì ìˆ˜ (25ì  ë§Œì )
        coverage_score = analysis["coverage_potential"] * 25
        score += coverage_score
        
        return round(score, 1)
    
    def _generate_quality_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """í’ˆì§ˆ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # í…ŒìŠ¤íŠ¸ ìˆ˜ëŸ‰ ê²€ì‚¬
        if analysis["total_tests"] < 5:
            recommendations.append("í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ 10ê°œ ì´ìƒ ì‘ì„±í•˜ì„¸ìš”.")
        
        # íƒ€ì… ë‹¤ì–‘ì„± ê²€ì‚¬
        test_types = analysis["test_types"]
        if len(test_types) < 2:
            recommendations.append("ë‹¤ì–‘í•œ íƒ€ì…ì˜ í…ŒìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš” (ë‹¨ìœ„, í†µí•©, E2E ë“±).")
        
        # ìš°ì„ ìˆœìœ„ ê· í˜• ê²€ì‚¬
        priorities = analysis["priority_distribution"]
        if "critical" not in priorities:
            recommendations.append("ì¤‘ìš”í•œ ê¸°ëŠ¥ì— ëŒ€í•œ CRITICAL ìš°ì„ ìˆœìœ„ í…ŒìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        
        # ì»¤ë²„ë¦¬ì§€ ì ì¬ë ¥ ê²€ì‚¬
        if analysis["coverage_potential"] < 0.7:
            recommendations.append("ë” ë§ì€ í•¨ìˆ˜ì™€ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì»¤ë²„ë¦¬ì§€ë¥¼ í–¥ìƒì‹œí‚¤ì„¸ìš”.")
        
        # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        quality_score = analysis["quality_score"]
        if quality_score < 60:
            recommendations.append("ì „ë°˜ì ì¸ í…ŒìŠ¤íŠ¸ í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤. ë” ì²´ê³„ì ì¸ í…ŒìŠ¤íŠ¸ ê³„íšì´ í•„ìš”í•©ë‹ˆë‹¤.")
        elif quality_score < 80:
            recommendations.append("í…ŒìŠ¤íŠ¸ í’ˆì§ˆì´ ì–‘í˜¸í•˜ì§€ë§Œ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
        
        return recommendations


class PerformanceTestGenerator:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def generate_performance_tests(self, function_analysis: Dict[str, Any], 
                                 performance_requirements: Optional[Dict] = None) -> List[TestCase]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìƒì„±"""
        function_name = function_analysis["name"]
        performance_requirements = performance_requirements or {}
        
        test_cases = []
        
        # ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        test_cases.append(self._generate_basic_performance_test(function_analysis))
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
        test_cases.append(self._generate_memory_test(function_analysis))
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        test_cases.append(self._generate_large_data_test(function_analysis))
        
        # ë™ì‹œì„± í…ŒìŠ¤íŠ¸
        test_cases.append(self._generate_concurrency_test(function_analysis))
        
        return test_cases
    
    def _generate_basic_performance_test(self, analysis: Dict[str, Any]) -> TestCase:
        """ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìƒì„±"""
        function_name = analysis["name"]
        parameters = analysis["parameters"]
        
        param_calls = []
        for param in parameters:
            default_val = self._get_performance_test_value(param.get("annotation", "Any"))
            param_calls.append(f"{param['name']}={repr(default_val)}")
        
        params_str = ", ".join(param_calls) if param_calls else ""
        
        test_code = f'''def test_{function_name}_performance():
    """
    {function_name} í•¨ìˆ˜ì˜ ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    """
    import time
    
    # Arrange
    iterations = 1000
    
    # Act
    start_time = time.time()
    for _ in range(iterations):
        result = {function_name}({params_str})
    end_time = time.time()
    
    # Assert
    total_time = end_time - start_time
    avg_time = total_time / iterations
    
    # í‰ê·  ì‹¤í–‰ ì‹œê°„ì´ 1ms ì´í•˜ì—¬ì•¼ í•¨
    assert avg_time < 0.001, f"Performance too slow: {{avg_time:.6f}}s per call"
    
    print(f"Average execution time: {{avg_time:.6f}}s")
    print(f"Total time for {{iterations}} iterations: {{total_time:.6f}}s")
'''
        
        return TestCase(
            name=f"test_{function_name}_performance",
            test_type=TestType.PERFORMANCE,
            function_name=function_name,
            test_code=test_code,
            description="ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸",
            priority=TestPriority.MEDIUM,
            tags=["performance", "speed"],
            expected_runtime=5.0
        )
    
    def _generate_memory_test(self, analysis: Dict[str, Any]) -> TestCase:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ìƒì„±"""
        function_name = analysis["name"]
        
        test_code = f'''def test_{function_name}_memory_usage():
    """
    {function_name} í•¨ìˆ˜ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
    """
    import tracemalloc
    import gc
    
    # Arrange
    gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìˆ˜í–‰
    tracemalloc.start()
    
    # Act
    initial_memory = tracemalloc.get_traced_memory()[0]
    
    # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
    for _ in range(100):
        result = {function_name}()  # TODO: ì ì ˆí•œ ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
    
    current_memory, peak_memory = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    # Assert
    memory_used = current_memory - initial_memory
    memory_per_call = memory_used / 100
    
    # í˜¸ì¶œë‹¹ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 1MB ì´í•˜ì—¬ì•¼ í•¨
    assert memory_per_call < 1024 * 1024, f"Memory usage too high: {{memory_per_call}} bytes per call"
    
    print(f"Memory per call: {{memory_per_call}} bytes")
    print(f"Peak memory: {{peak_memory}} bytes")
'''
        
        return TestCase(
            name=f"test_{function_name}_memory_usage",
            test_type=TestType.PERFORMANCE,
            function_name=function_name,
            test_code=test_code,
            description="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸",
            priority=TestPriority.LOW,
            tags=["performance", "memory"]
        )
    
    def _generate_large_data_test(self, analysis: Dict[str, Any]) -> TestCase:
        """ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ìƒì„±"""
        function_name = analysis["name"]
        
        test_code = f'''def test_{function_name}_large_data():
    """
    {function_name} í•¨ìˆ˜ì˜ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    """
    import time
    
    # Arrange
    large_data = list(range(10000))  # TODO: í•¨ìˆ˜ì— ë§ëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒì„±
    
    # Act
    start_time = time.time()
    result = {function_name}(large_data)  # TODO: ì ì ˆí•œ ë§¤ê°œë³€ìˆ˜ ìˆ˜ì •
    end_time = time.time()
    
    # Assert
    execution_time = end_time - start_time
    
    # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ê°€ 10ì´ˆ ì´í•˜ì—¬ì•¼ í•¨
    assert execution_time < 10.0, f"Large data processing too slow: {{execution_time:.2f}}s"
    
    print(f"Large data processing time: {{execution_time:.2f}}s")
'''
        
        return TestCase(
            name=f"test_{function_name}_large_data",
            test_type=TestType.PERFORMANCE,
            function_name=function_name,
            test_code=test_code,
            description="ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸",
            priority=TestPriority.MEDIUM,
            tags=["performance", "large_data"]
        )
    
    def _generate_concurrency_test(self, analysis: Dict[str, Any]) -> TestCase:
        """ë™ì‹œì„± í…ŒìŠ¤íŠ¸ ìƒì„±"""
        function_name = analysis["name"]
        
        test_code = f'''def test_{function_name}_concurrency():
    """
    {function_name} í•¨ìˆ˜ì˜ ë™ì‹œì„± í…ŒìŠ¤íŠ¸
    """
    import threading
    import time
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    # Arrange
    num_threads = 10
    iterations_per_thread = 100
    results = []
    errors = []
    
    def worker():
        try:
            for _ in range(iterations_per_thread):
                result = {function_name}()  # TODO: ì ì ˆí•œ ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
                results.append(result)
        except Exception as e:
            errors.append(str(e))
    
    # Act
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = [executor.submit(worker) for _ in range(num_threads)]
        for future in as_completed(futures):
            future.result()  # ì˜ˆì™¸ ë°œìƒ ì‹œ ì—¬ê¸°ì„œ ì²˜ë¦¬
    
    end_time = time.time()
    
    # Assert
    total_time = end_time - start_time
    total_operations = num_threads * iterations_per_thread
    
    assert len(errors) == 0, f"Concurrency errors occurred: {{errors[:5]}}"
    assert len(results) == total_operations, f"Expected {{total_operations}} results, got {{len(results)}}"
    
    print(f"Concurrent execution time: {{total_time:.2f}}s")
    print(f"Operations per second: {{total_operations / total_time:.2f}}")
'''
        
        return TestCase(
            name=f"test_{function_name}_concurrency",
            test_type=TestType.PERFORMANCE,
            function_name=function_name,
            test_code=test_code,
            description="ë™ì‹œì„± í…ŒìŠ¤íŠ¸",
            priority=TestPriority.MEDIUM,
            tags=["performance", "concurrency", "threading"]
        )
    
    def _get_performance_test_value(self, param_type: str) -> Any:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© ë§¤ê°œë³€ìˆ˜ ê°’ ìƒì„±"""
        if "int" in param_type:
            return 1000
        elif "str" in param_type:
            return "performance_test_string"
        elif "list" in param_type:
            return list(range(100))
        elif "dict" in param_type:
            return {f"key_{i}": f"value_{i}" for i in range(100)}
        else:
            return "test_value"


class TestReportGenerator:
    """í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def generate_comprehensive_report(self, 
                                    test_results: List[TestResult],
                                    coverage_report: Optional[CoverageReport] = None,
                                    quality_analysis: Optional[Dict] = None) -> str:
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        
        report_lines = [
            "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë³´ê³ ì„œ",
            f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## ğŸ“Š ì‹¤í–‰ ê²°ê³¼ ìš”ì•½"
        ]
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        if test_results:
            total_tests = len(test_results)
            passed = len([r for r in test_results if r.status == "passed"])
            failed = len([r for r in test_results if r.status == "failed"])
            errors = len([r for r in test_results if r.status == "error"])
            skipped = len([r for r in test_results if r.status == "skipped"])
            
            success_rate = (passed / total_tests * 100) if total_tests > 0 else 0
            total_time = sum(r.execution_time for r in test_results)
            
            report_lines.extend([
                f"- **ì´ í…ŒìŠ¤íŠ¸ ìˆ˜**: {total_tests}",
                f"- **ì„±ê³µ**: {passed} ({success_rate:.1f}%)",
                f"- **ì‹¤íŒ¨**: {failed}",
                f"- **ì˜¤ë¥˜**: {errors}",
                f"- **ê±´ë„ˆëœ€**: {skipped}",
                f"- **ì´ ì‹¤í–‰ ì‹œê°„**: {total_time:.2f}ì´ˆ",
                ""
            ])
        
        # ì»¤ë²„ë¦¬ì§€ ì •ë³´
        if coverage_report:
            report_lines.extend([
                "## ğŸ¯ ì»¤ë²„ë¦¬ì§€ ë¶„ì„",
                f"- **ë¼ì¸ ì»¤ë²„ë¦¬ì§€**: {coverage_report.line_coverage:.1%}",
                f"- **ë¸Œëœì¹˜ ì»¤ë²„ë¦¬ì§€**: {coverage_report.branch_coverage:.1%}",
                f"- **í•¨ìˆ˜ ì»¤ë²„ë¦¬ì§€**: {coverage_report.function_coverage:.1%}",
                f"- **ì´ ë¼ì¸ ìˆ˜**: {coverage_report.total_lines}",
                f"- **ì»¤ë²„ëœ ë¼ì¸ ìˆ˜**: {coverage_report.covered_lines}",
                ""
            ])
            
            if coverage_report.missing_lines:
                report_lines.extend([
                    "### ë¯¸ì»¤ë²„ ë¼ì¸",
                    f"ë‹¤ìŒ {len(coverage_report.missing_lines)}ê°œ ë¼ì¸ì´ í…ŒìŠ¤íŠ¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤:",
                    ", ".join(map(str, coverage_report.missing_lines[:20])),
                    "..." if len(coverage_report.missing_lines) > 20 else "",
                    ""
                ])
        
        # í’ˆì§ˆ ë¶„ì„
        if quality_analysis:
            report_lines.extend([
                "## ğŸ† í…ŒìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„",
                f"- **í’ˆì§ˆ ì ìˆ˜**: {quality_analysis.get('quality_score', 0)}/100",
                f"- **ì´ í…ŒìŠ¤íŠ¸ ìˆ˜**: {quality_analysis.get('total_tests', 0)}",
                f"- **ì»¤ë²„ë¦¬ì§€ ì ì¬ë ¥**: {quality_analysis.get('coverage_potential', 0):.1%}",
                ""
            ])
            
            recommendations = quality_analysis.get('recommendations', [])
            if recommendations:
                report_lines.extend([
                    "### ê°œì„  ê¶Œì¥ì‚¬í•­",
                    *[f"- {rec}" for rec in recommendations],
                    ""
                ])
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„¸ ì •ë³´
        failed_tests = [r for r in test_results if r.status in ["failed", "error"]]
        if failed_tests:
            report_lines.extend([
                "## âŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„¸",
                ""
            ])
            
            for test in failed_tests:
                report_lines.extend([
                    f"### {test.test_name}",
                    f"- **ìƒíƒœ**: {test.status}",
                    f"- **ì‹¤í–‰ ì‹œê°„**: {test.execution_time:.3f}ì´ˆ",
                    f"- **ì˜¤ë¥˜ ë©”ì‹œì§€**: {test.error_message or 'N/A'}",
                    ""
                ])
        
        # ì„±ëŠ¥ í†µê³„
        performance_tests = [r for r in test_results if "performance" in r.test_name.lower()]
        if performance_tests:
            avg_perf_time = sum(r.execution_time for r in performance_tests) / len(performance_tests)
            report_lines.extend([
                "## âš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼",
                f"- **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìˆ˜**: {len(performance_tests)}",
                f"- **í‰ê·  ì‹¤í–‰ ì‹œê°„**: {avg_perf_time:.3f}ì´ˆ",
                ""
            ])
        
        return "\n".join(report_lines)
    
    def generate_html_report(self, 
                           test_results: List[TestResult],
                           coverage_report: Optional[CoverageReport] = None) -> str:
        """HTML í˜•ì‹ ë³´ê³ ì„œ ìƒì„±"""
        
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        total_tests = len(test_results)
        passed = len([r for r in test_results if r.status == "passed"])
        failed = len([r for r in test_results if r.status == "failed"])
        success_rate = (passed / total_tests * 100) if total_tests > 0 else 0
        
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë³´ê³ ì„œ</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .summary {{ background: #f5f5f5; padding: 20px; border-radius: 8px; margin-bottom: 30px; }}
        .success {{ color: #28a745; }}
        .failure {{ color: #dc3545; }}
        .test-result {{ border: 1px solid #ddd; margin: 10px 0; padding: 15px; border-radius: 5px; }}
        .test-passed {{ border-left: 5px solid #28a745; }}
        .test-failed {{ border-left: 5px solid #dc3545; }}
        .coverage-bar {{ width: 100%; height: 20px; background: #ddd; border-radius: 10px; overflow: hidden; }}
        .coverage-fill {{ height: 100%; background: linear-gradient(to right, #dc3545, #ffc107, #28a745); }}
    </style>
</head>
<body>
    <h1>í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë³´ê³ ì„œ</h1>
    <p>ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    
    <div class="summary">
        <h2>ì‹¤í–‰ ê²°ê³¼ ìš”ì•½</h2>
        <p><strong>ì´ í…ŒìŠ¤íŠ¸:</strong> {total_tests}</p>
        <p><strong>ì„±ê³µë¥ :</strong> <span class="{'success' if success_rate >= 80 else 'failure'}">{success_rate:.1f}%</span></p>
        <p><strong>ì„±ê³µ:</strong> <span class="success">{passed}</span></p>
        <p><strong>ì‹¤íŒ¨:</strong> <span class="failure">{failed}</span></p>
    </div>
"""
        
        # ì»¤ë²„ë¦¬ì§€ ì •ë³´ ì¶”ê°€
        if coverage_report:
            coverage_percent = coverage_report.line_coverage * 100
            html += f"""
    <div class="summary">
        <h2>ì»¤ë²„ë¦¬ì§€ ì •ë³´</h2>
        <p><strong>ë¼ì¸ ì»¤ë²„ë¦¬ì§€:</strong> {coverage_percent:.1f}%</p>
        <div class="coverage-bar">
            <div class="coverage-fill" style="width: {coverage_percent}%;"></div>
        </div>
    </div>
"""
        
        # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        html += "<h2>í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„¸</h2>"
        
        for test in test_results:
            status_class = "test-passed" if test.status == "passed" else "test-failed"
            html += f"""
    <div class="test-result {status_class}">
        <h3>{test.test_name}</h3>
        <p><strong>ìƒíƒœ:</strong> {test.status}</p>
        <p><strong>ì‹¤í–‰ ì‹œê°„:</strong> {test.execution_time:.3f}ì´ˆ</p>
        {f'<p><strong>ì˜¤ë¥˜:</strong> {test.error_message}</p>' if test.error_message else ''}
    </div>
"""
        
        html += "</body></html>"
        return html


class TestingAssistant:
    """í†µí•© í…ŒìŠ¤íŒ… ì–´ì‹œìŠ¤í„´íŠ¸"""
    
    def __init__(self, framework: TestFramework = TestFramework.PYTEST):
        self.framework = framework
        self.unit_generator = UnitTestGenerator(framework)
        self.integration_generator = IntegrationTestGenerator()
        self.performance_generator = PerformanceTestGenerator()
        self.coverage_analyzer = CoverageAnalyzer()
        self.test_executor = TestExecutor(framework)
        self.quality_analyzer = TestQualityAnalyzer()
        self.report_generator = TestReportGenerator()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def generate_comprehensive_test_suite(self, 
                                        function_code: str,
                                        include_performance: bool = True,
                                        include_integration: bool = False,
                                        additional_context: Optional[Dict] = None) -> TestSuite:
        """í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ìƒì„±"""
        try:
            all_test_cases = []
            
            # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ìƒì„±
            unit_tests = self.unit_generator.generate_tests(function_code, additional_context)
            all_test_cases.extend(unit_tests)
            
            # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìƒì„±
            if include_performance:
                function_analyzer = FunctionAnalyzer()
                analysis = function_analyzer.analyze_function(function_code)
                if "error" not in analysis:
                    perf_tests = self.performance_generator.generate_performance_tests(analysis)
                    all_test_cases.extend(perf_tests)
            
            # í†µí•© í…ŒìŠ¤íŠ¸ ìƒì„± (í•„ìš”ì‹œ)
            if include_integration and additional_context:
                modules = additional_context.get("modules", [])
                interactions = additional_context.get("interactions", [])
                if modules and interactions:
                    integration_tests = self.integration_generator.generate_integration_tests(
                        modules, interactions
                    )
                    all_test_cases.extend(integration_tests)
            
            # í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ìƒì„±
            function_analyzer = FunctionAnalyzer()
            analysis = function_analyzer.analyze_function(function_code)
            function_name = analysis.get("name", "unknown_function")
            
            test_suite = TestSuite(
                name=f"{function_name}_test_suite",
                test_cases=all_test_cases,
                parallel_execution=True,
                timeout=60.0
            )
            
            self.logger.info(f"í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ìƒì„± ì™„ë£Œ: {len(all_test_cases)}ê°œ í…ŒìŠ¤íŠ¸")
            return test_suite
            
        except Exception as e:
            self.logger.error(f"í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise TestGenerationError(f"í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def run_full_analysis(self, 
                         function_code: str,
                         source_files: Optional[List[str]] = None,
                         run_tests: bool = True) -> Dict[str, Any]:
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ë¶„ì„ ì‹¤í–‰"""
        try:
            # 1. í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ìƒì„±
            test_suite = self.generate_comprehensive_test_suite(function_code)
            
            # 2. í…ŒìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„
            quality_analysis = self.quality_analyzer.analyze_test_quality(test_suite.test_cases)
            
            results = {
                "test_suite": test_suite,
                "quality_analysis": quality_analysis,
                "test_results": None,
                "coverage_report": None,
                "recommendations": []
            }
            
            # 3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì˜µì…˜)
            if run_tests:
                test_results = self.test_executor.execute_tests(test_suite, parallel=True)
                results["test_results"] = test_results
                
                # 4. ì»¤ë²„ë¦¬ì§€ ë¶„ì„ (ì†ŒìŠ¤ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)
                if source_files:
                    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì„ì‹œ ìƒì„±
                    test_files = self._create_temp_test_files(test_suite.test_cases)
                    try:
                        coverage_report = self.coverage_analyzer.analyze_coverage(
                            source_files, test_files
                        )
                        results["coverage_report"] = coverage_report
                        
                        # ì»¤ë²„ë¦¬ì§€ ê°œì„  ì œì•ˆ
                        coverage_suggestions = self.coverage_analyzer.suggest_coverage_improvements(
                            coverage_report
                        )
                        results["recommendations"].extend(coverage_suggestions)
                        
                    finally:
                        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                        self._cleanup_temp_files(test_files)
            
            # 5. ì¢…í•© ê¶Œì¥ì‚¬í•­
            results["recommendations"].extend(quality_analysis.get("recommendations", []))
            
            return results
            
        except Exception as e:
            self.logger.error(f"ì „ì²´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise TestExecutionError(f"ì „ì²´ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _create_temp_test_files(self, test_cases: List[TestCase]) -> List[str]:
        """ì„ì‹œ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"""
        temp_files = []
        
        try:
            for i, test_case in enumerate(test_cases):
                temp_file = tempfile.NamedTemporaryFile(
                    mode='w', 
                    suffix='.py', 
                    prefix=f'test_{i}_',
                    delete=False
                )
                
                # í•„ìš”í•œ import ì¶”ê°€
                imports = [
                    "import pytest",
                    "import unittest",
                    "from unittest.mock import Mock, patch"
                ]
                
                content = "\n".join(imports) + "\n\n" + test_case.test_code
                temp_file.write(content)
                temp_file.close()
                
                temp_files.append(temp_file.name)
            
            return temp_files
            
        except Exception as e:
            # ìƒì„±ëœ ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬
            self._cleanup_temp_files(temp_files)
            raise e
    
    def _cleanup_temp_files(self, file_paths: List[str]):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        for file_path in file_paths:
            try:
                Path(file_path).unlink()
            except Exception as e:
                self.logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path}, {e}")
    
    def generate_test_report(self, analysis_results: Dict[str, Any], 
                           format: str = "markdown") -> str:
        """í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        try:
            test_results = analysis_results.get("test_results", [])
            coverage_report = analysis_results.get("coverage_report")
            quality_analysis = analysis_results.get("quality_analysis")
            
            if format == "html":
                return self.report_generator.generate_html_report(
                    test_results, coverage_report
                )
            else:
                return self.report_generator.generate_comprehensive_report(
                    test_results, coverage_report, quality_analysis
                )
                
        except Exception as e:
            self.logger.error(f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}"


# í¸ì˜ í•¨ìˆ˜ë“¤
def generate_unit_tests(function_code: str, 
                       framework: TestFramework = TestFramework.PYTEST) -> List[TestCase]:
    """ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    generator = UnitTestGenerator(framework)
    return generator.generate_tests(function_code)


def analyze_test_coverage(source_files: List[str], 
                         test_files: List[str]) -> CoverageReport:
    """í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë¶„ì„ í¸ì˜ í•¨ìˆ˜"""
    analyzer = CoverageAnalyzer()
    return analyzer.analyze_coverage(source_files, test_files)


def generate_performance_tests(function_code: str) -> List[TestCase]:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    analyzer = FunctionAnalyzer()
    analysis = analyzer.analyze_function(function_code)
    
    if "error" in analysis:
        raise TestGenerationError(f"í•¨ìˆ˜ ë¶„ì„ ì‹¤íŒ¨: {analysis['error']}")
    
    generator = PerformanceTestGenerator()
    return generator.generate_performance_tests(analysis)


def create_testing_assistant(framework: TestFramework = TestFramework.PYTEST) -> TestingAssistant:
    """í…ŒìŠ¤íŒ… ì–´ì‹œìŠ¤í„´íŠ¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return TestingAssistant(framework)


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # ì˜ˆì œ í•¨ìˆ˜
    sample_function = '''
def calculate_fibonacci(n: int) -> int:
    """í”¼ë³´ë‚˜ì¹˜ ìˆ˜ ê³„ì‚°"""
    if n < 0:
        raise ValueError("ìŒìˆ˜ëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
    elif n <= 1:
        return n
    else:
        return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)
'''
    
    # í…ŒìŠ¤íŒ… ì–´ì‹œìŠ¤í„´íŠ¸ ìƒì„±
    assistant = create_testing_assistant(TestFramework.PYTEST)
    
    # ì „ì²´ ë¶„ì„ ì‹¤í–‰
    print("=== í…ŒìŠ¤íŠ¸ ë¶„ì„ ì‹œì‘ ===")
    results = assistant.run_full_analysis(sample_function, run_tests=False)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ìˆ˜: {len(results['test_suite'].test_cases)}")
    print(f"í’ˆì§ˆ ì ìˆ˜: {results['quality_analysis']['quality_score']}/100")
    
    # ë³´ê³ ì„œ ìƒì„±
    report = assistant.generate_test_report(results)
    print("\n=== í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ===")
    print(report)